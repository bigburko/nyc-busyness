{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 RUNNING ML-OPTIMIZED PROCESSOR\n",
      "Goal: 0-10 decimal scores + taxi/subway combination + ML-ready temporal patterns\n",
      "Data: 2020-2023 (4 years) | Combination: 65% taxi + 35% subway\n",
      "📂 Run from: foot_traffic_score/ directory\n",
      "\n",
      "🤖 ML-Optimized Foot Traffic Processor\n",
      "🎯 Goal: 0-10 scale (×10 on frontend) + proper time series ML\n",
      "🚇 Includes: Taxi (65%) + Subway (35%) combination\n",
      "📅 Data: 2020-2023 (4 years for robust ML training)\n",
      "======================================================================\n",
      "📊 STEP 1: Collecting ALL raw trip counts...\n",
      "   📅 Processing 2020... "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "def ml_optimized_foot_traffic_processor():\n",
    "    \"\"\"\n",
    "    ML-OPTIMIZED VERSION: Global normalization for time series prediction\n",
    "    INCLUDES: Taxi foot traffic + subway scores combination (65%/35% weighting)\n",
    "    \n",
    "    Key insight: ALL data (all years, all periods) normalized together\n",
    "    This preserves temporal relationships crucial for ML learning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🤖 ML-Optimized Foot Traffic Processor\")\n",
    "    print(\"🎯 Goal: 0-10 scale (×10 on frontend) + proper time series ML\")\n",
    "    print(\"🚇 Includes: Taxi (65%) + Subway (35%) combination\")\n",
    "    print(\"📅 Data: 2020-2023 (4 years for robust ML training)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    files = {\n",
    "        2020: 'YellowTaxiYears/2020_Yellow_Taxi_Trip_Data.csv',\n",
    "        2021: 'YellowTaxiYears/2021_Yellow_Taxi_Trip_Data.csv', \n",
    "        2022: 'YellowTaxiYears/2022_Yellow_Taxi_Trip_Data.csv',\n",
    "        2023: 'YellowTaxiYears/2023_Yellow_Taxi_Trip_Data.csv'\n",
    "    }\n",
    "    \n",
    "    manhattan_zones = [4, 12, 13, 14, 24, 41, 42, 43, 45, 48, 50, 68, 74, 75, 79, 87, 88, 90, 100, 107, 113, 114, 116, 125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166, 170, 186, 230, 231, 232, 233, 234]\n",
    "    \n",
    "    # STEP 1: Collect ALL raw counts (every year, every period, every zone)\n",
    "    print(\"📊 STEP 1: Collecting ALL raw trip counts...\")\n",
    "    \n",
    "    all_raw_pickup_counts = []\n",
    "    all_raw_dropoff_counts = []\n",
    "    raw_data_store = {}\n",
    "    \n",
    "    for year, file in files.items():\n",
    "        print(f\"   📅 Processing {year}...\", end=\" \")\n",
    "        \n",
    "        # Load and filter data\n",
    "        df = pd.read_csv(file, usecols=['tpep_pickup_datetime', 'PULocationID', 'DOLocationID'])\n",
    "        df['hour'] = pd.to_datetime(df['tpep_pickup_datetime'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce').dt.hour\n",
    "        df = df.dropna(subset=['hour'])\n",
    "        df = df[df['PULocationID'].isin(manhattan_zones) & df['DOLocationID'].isin(manhattan_zones)]\n",
    "        \n",
    "        # Define time periods\n",
    "        periods = {\n",
    "            'morning': (df['hour'] >= 6) & (df['hour'] < 12),\n",
    "            'afternoon': (df['hour'] >= 12) & (df['hour'] < 18),\n",
    "            'evening': (df['hour'] >= 18) & (df['hour'] < 24),\n",
    "            'night': (df['hour'] >= 0) & (df['hour'] < 6)\n",
    "        }\n",
    "        \n",
    "        year_pickup_counts = []\n",
    "        year_dropoff_counts = []\n",
    "        \n",
    "        # Process each time period\n",
    "        for period_name, period_mask in periods.items():\n",
    "            period_df = df[period_mask]\n",
    "            \n",
    "            # Get raw counts\n",
    "            pickups = period_df.groupby('PULocationID').size().reindex(manhattan_zones, fill_value=0).values\n",
    "            dropoffs = period_df.groupby('DOLocationID').size().reindex(manhattan_zones, fill_value=0).values\n",
    "            \n",
    "            # Store for later use\n",
    "            raw_data_store[f'{period_name}_{year}_pickup'] = pickups\n",
    "            raw_data_store[f'{period_name}_{year}_dropoff'] = dropoffs\n",
    "            \n",
    "            # Add to global collection\n",
    "            all_raw_pickup_counts.extend(pickups)\n",
    "            all_raw_dropoff_counts.extend(dropoffs)\n",
    "            year_pickup_counts.extend(pickups)\n",
    "            year_dropoff_counts.extend(dropoffs)\n",
    "        \n",
    "        # Overall year counts\n",
    "        all_pickups = df.groupby('PULocationID').size().reindex(manhattan_zones, fill_value=0).values\n",
    "        all_dropoffs = df.groupby('DOLocationID').size().reindex(manhattan_zones, fill_value=0).values\n",
    "        \n",
    "        raw_data_store[f'average_{year}_pickup'] = all_pickups\n",
    "        raw_data_store[f'average_{year}_dropoff'] = all_dropoffs\n",
    "        \n",
    "        all_raw_pickup_counts.extend(all_pickups)\n",
    "        all_raw_dropoff_counts.extend(all_dropoffs)\n",
    "        \n",
    "        print(f\"✅ {len(df):,} trips, {len(year_pickup_counts)} data points\")\n",
    "    \n",
    "    # STEP 2: Global normalization strategy\n",
    "    print(f\"\\n🔄 STEP 2: GLOBAL normalization across all data...\")\n",
    "    \n",
    "    print(f\"   📈 Raw data stats:\")\n",
    "    pickup_array = np.array(all_raw_pickup_counts)\n",
    "    dropoff_array = np.array(all_raw_dropoff_counts)\n",
    "    \n",
    "    print(f\"      Pickups: min={pickup_array.min()}, max={pickup_array.max()}, mean={pickup_array.mean():.1f}\")\n",
    "    print(f\"      Dropoffs: min={dropoff_array.min()}, max={dropoff_array.max()}, mean={dropoff_array.mean():.1f}\")\n",
    "    \n",
    "    # Strategy: MinMaxScaler to 0-10 (frontend will multiply by 10 for display)\n",
    "    print(f\"   🎯 Applying global 0-10 normalization...\")\n",
    "    \n",
    "    pickup_scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "    dropoff_scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "    \n",
    "    # Fit on ALL data\n",
    "    pickup_scaler.fit(pickup_array.reshape(-1, 1))\n",
    "    dropoff_scaler.fit(dropoff_array.reshape(-1, 1))\n",
    "    \n",
    "    # STEP 3: Apply normalization and create foot traffic scores\n",
    "    print(f\"\\n⚡ STEP 3: Creating foot traffic scores...\")\n",
    "    \n",
    "    results = {'id': range(1, len(manhattan_zones) + 1), 'GEOID': manhattan_zones}\n",
    "    \n",
    "    years = list(files.keys())\n",
    "    periods = ['morning', 'afternoon', 'evening', 'night']\n",
    "    \n",
    "    # Process each year and period\n",
    "    for year in years:\n",
    "        for period in periods:\n",
    "            # Get raw counts\n",
    "            pickups = raw_data_store[f'{period}_{year}_pickup']\n",
    "            dropoffs = raw_data_store[f'{period}_{year}_dropoff']\n",
    "            \n",
    "            # Apply GLOBAL normalization\n",
    "            pickup_scaled = pickup_scaler.transform(pickups.reshape(-1, 1)).flatten()\n",
    "            dropoff_scaled = dropoff_scaler.transform(dropoffs.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Calculate foot traffic score (0.7 dropoff + 0.3 pickup)\n",
    "            foot_traffic_score = 0.7 * dropoff_scaled + 0.3 * pickup_scaled\n",
    "            results[f'{period}_{year}'] = foot_traffic_score\n",
    "        \n",
    "        # Process average\n",
    "        pickups = raw_data_store[f'average_{year}_pickup']\n",
    "        dropoffs = raw_data_store[f'average_{year}_dropoff']\n",
    "        \n",
    "        pickup_scaled = pickup_scaler.transform(pickups.reshape(-1, 1)).flatten()\n",
    "        dropoff_scaled = dropoff_scaler.transform(dropoffs.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        avg_score = 0.7 * dropoff_scaled + 0.3 * pickup_scaled\n",
    "        results[f'average_{year}'] = avg_score\n",
    "    \n",
    "    # STEP 4: Spatial mapping from taxi zones to census tracts\n",
    "    print(f\"\\n🗺️  STEP 4: Spatial mapping taxi zones → census tracts...\")\n",
    "    \n",
    "    try:\n",
    "        # Load taxi zones shapefile - checking multiple possible locations\n",
    "        taxi_zones_paths = [\n",
    "            \"taxi_zones/taxi_zones.shp\",  # Original path\n",
    "            \"../taxi_zones/taxi_zones.shp\",  # One level up\n",
    "            \"../../taxi_zones/taxi_zones.shp\"  # Two levels up\n",
    "        ]\n",
    "        \n",
    "        taxi_zones = None\n",
    "        for path in taxi_zones_paths:\n",
    "            try:\n",
    "                taxi_zones = gpd.read_file(path)\n",
    "                taxi_zones = taxi_zones[taxi_zones[\"borough\"] == \"Manhattan\"].copy()\n",
    "                print(f\"   ✅ Loaded {len(taxi_zones)} Manhattan taxi zones from {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if taxi_zones is None:\n",
    "            raise FileNotFoundError(\"Taxi zones shapefile not found in any expected location\")\n",
    "        \n",
    "        # Load census tracts (your exact structure)\n",
    "        census_tracts = gpd.read_file(\"../census tract geofiles/manhattan_census_tracts.geojson\")\n",
    "        census_tracts = census_tracts[[\"GEOID\", \"geometry\"]].to_crs(taxi_zones.crs)\n",
    "        print(f\"   ✅ Loaded {len(census_tracts)} Manhattan census tracts\")\n",
    "        \n",
    "        # Create mapping from taxi zones to census tracts using spatial overlays\n",
    "        print(f\"   🔄 Computing spatial overlaps...\")\n",
    "        \n",
    "        # Method: Use spatial intersection to find overlaps\n",
    "        overlaps = gpd.overlay(taxi_zones, census_tracts, how='intersection')\n",
    "        overlaps['overlap_area'] = overlaps.geometry.area\n",
    "        \n",
    "        # For each taxi zone, find which census tracts it overlaps with\n",
    "        zone_to_tract_mapping = []\n",
    "        \n",
    "        for location_id in manhattan_zones:\n",
    "            zone_overlaps = overlaps[overlaps['LocationID'] == location_id]\n",
    "            \n",
    "            if len(zone_overlaps) > 0:\n",
    "                # Get the tract(s) with the largest overlap area\n",
    "                total_area = zone_overlaps['overlap_area'].sum()\n",
    "                \n",
    "                for _, overlap in zone_overlaps.iterrows():\n",
    "                    # Weight by overlap area\n",
    "                    weight = overlap['overlap_area'] / total_area if total_area > 0 else 1.0\n",
    "                    zone_to_tract_mapping.append({\n",
    "                        'LocationID': location_id,\n",
    "                        'GEOID': overlap['GEOID'],\n",
    "                        'weight': weight\n",
    "                    })\n",
    "            else:\n",
    "                # Fallback: use nearest tract\n",
    "                taxi_zone = taxi_zones[taxi_zones['LocationID'] == location_id]\n",
    "                if len(taxi_zone) > 0:\n",
    "                    zone_centroid = taxi_zone.geometry.centroid.iloc[0]\n",
    "                    distances = census_tracts.geometry.distance(zone_centroid)\n",
    "                    nearest_geoid = census_tracts.iloc[distances.idxmin()]['GEOID']\n",
    "                    \n",
    "                    zone_to_tract_mapping.append({\n",
    "                        'LocationID': location_id,\n",
    "                        'GEOID': nearest_geoid,\n",
    "                        'weight': 1.0\n",
    "                    })\n",
    "        \n",
    "        mapping_df = pd.DataFrame(zone_to_tract_mapping)\n",
    "        print(f\"   ✅ Created {len(mapping_df)} zone→tract mappings\")\n",
    "        \n",
    "        # Apply spatial mapping to create tract-level scores\n",
    "        print(f\"   🔄 Aggregating taxi scores by census tract...\")\n",
    "        \n",
    "        tract_results = {'GEOID': []}\n",
    "        years = list(files.keys())\n",
    "        \n",
    "        # Get all unique GEOIDs\n",
    "        unique_geoids = mapping_df['GEOID'].unique()\n",
    "        tract_results['GEOID'] = unique_geoids\n",
    "        tract_results['id'] = range(1, len(unique_geoids) + 1)\n",
    "        \n",
    "        # For each time period and year, aggregate scores by tract\n",
    "        periods = ['morning', 'afternoon', 'evening', 'night', 'average']\n",
    "        \n",
    "        for period in periods:\n",
    "            for year in years:\n",
    "                col_name = f'{period}_{year}'\n",
    "                tract_scores = []\n",
    "                \n",
    "                for geoid in unique_geoids:\n",
    "                    # Get all taxi zones that map to this tract\n",
    "                    zone_mappings = mapping_df[mapping_df['GEOID'] == geoid]\n",
    "                    \n",
    "                    # Calculate weighted average of taxi scores\n",
    "                    weighted_score = 0\n",
    "                    total_weight = 0\n",
    "                    \n",
    "                    for _, mapping in zone_mappings.iterrows():\n",
    "                        location_id = mapping['LocationID']\n",
    "                        weight = mapping['weight']\n",
    "                        \n",
    "                        # Find the score for this LocationID\n",
    "                        zone_idx = manhattan_zones.index(location_id)\n",
    "                        zone_score = results[col_name][zone_idx]\n",
    "                        \n",
    "                        weighted_score += zone_score * weight\n",
    "                        total_weight += weight\n",
    "                    \n",
    "                    # Final score for this tract\n",
    "                    final_score = weighted_score / total_weight if total_weight > 0 else 0\n",
    "                    tract_scores.append(final_score)\n",
    "                \n",
    "                tract_results[col_name] = tract_scores\n",
    "        \n",
    "        # Replace results with tract-based data\n",
    "        final_df = pd.DataFrame(tract_results)\n",
    "        print(f\"   ✅ Created tract-based foot traffic scores: {len(final_df)} census tracts\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   ⚠️  Spatial files not found: {e}\")\n",
    "        print(f\"   📝 Using simple LocationID→GEOID mapping instead...\")\n",
    "        \n",
    "        # Fallback: Simple mapping like your project  \n",
    "        try:\n",
    "            import json\n",
    "            # Use your exact file structure\n",
    "            with open('../census tract geofiles/manhattan_census_tracts.geojson', 'r') as f:\n",
    "                geojson = json.load(f)\n",
    "            geoids = [feature['properties']['GEOID'] for feature in geojson['features']]\n",
    "            \n",
    "            # Simple distribution approach (like your project)\n",
    "            result_rows = []\n",
    "            locations = sorted(manhattan_zones)\n",
    "            tracts_per_location = len(geoids) // len(locations)\n",
    "            \n",
    "            print(f\"   📊 Mapping {len(locations)} LocationIDs to {len(geoids)} GEOIDs\")\n",
    "            print(f\"   📈 Approximately {tracts_per_location} census tracts per LocationID\")\n",
    "            \n",
    "            geoid_index = 0\n",
    "            for i, location_id in enumerate(locations):\n",
    "                num_geoids = tracts_per_location + (1 if i < len(geoids) % len(locations) else 0)\n",
    "                location_idx = manhattan_zones.index(location_id)\n",
    "                \n",
    "                for j in range(num_geoids):\n",
    "                    if geoid_index < len(geoids):\n",
    "                        row = {'GEOID': geoids[geoid_index], 'id': geoid_index + 1}\n",
    "                        \n",
    "                        # Copy all scores from this LocationID\n",
    "                        for col, values in results.items():\n",
    "                            if col not in ['id', 'GEOID']:\n",
    "                                row[col] = values[location_idx]\n",
    "                        \n",
    "                        result_rows.append(row)\n",
    "                        geoid_index += 1\n",
    "            \n",
    "            final_df = pd.DataFrame(result_rows)\n",
    "            print(f\"   ✅ Created simple mapping: {len(final_df)} census tracts\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ❌ Fallback mapping failed: {e2}\")\n",
    "            print(f\"   📝 Keeping taxi LocationIDs as GEOID\")\n",
    "            final_df = pd.DataFrame(results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Spatial mapping error: {e}\")\n",
    "        print(f\"   📝 Keeping taxi LocationIDs as GEOID\")\n",
    "        final_df = pd.DataFrame(results)\n",
    "    \n",
    "    # STEP 5: Load subway scores and create combined scores\n",
    "    print(f\"\\n🚇 STEP 5: Loading subway scores and creating combined scores...\")\n",
    "    \n",
    "    try:\n",
    "        # Load subway scores by census tract\n",
    "        subway_scores = pd.read_csv('subway_score_by_tract.csv')\n",
    "        print(f\"   ✅ Loaded subway scores: {len(subway_scores)} census tracts\")\n",
    "        \n",
    "        # Create zone-to-tract mapping (simplified - you may need a proper mapping file)\n",
    "        # For now, assume each taxi zone maps to a census tract with similar ID patterns\n",
    "        # You can replace this with actual spatial mapping if needed\n",
    "        final_df['GEOID'] = final_df['GEOID'].astype(str)\n",
    "        subway_scores['GEOID'] = subway_scores['GEOID'].astype(str)\n",
    "        \n",
    "        # Merge subway scores\n",
    "        final_df = final_df.merge(subway_scores, on='GEOID', how='left')\n",
    "        final_df['subway_score'] = final_df['subway_score'].fillna(0)\n",
    "        \n",
    "        # Create combined scores for each time period and year\n",
    "        score_columns = [col for col in final_df.columns \n",
    "                        if col.endswith(tuple(str(y) for y in years)) \n",
    "                        and col != 'subway_score']\n",
    "        \n",
    "        for col in score_columns:\n",
    "            combined_col = col.replace('_', '_combined_')\n",
    "            final_df[combined_col] = (\n",
    "                0.65 * final_df[col] + \n",
    "                0.35 * final_df['subway_score']\n",
    "            ).round(3)\n",
    "        \n",
    "        print(f\"   🔄 Created {len(score_columns)} combined scores: taxi (65%) + subway (35%)\")\n",
    "        print(f\"   📊 Combined score range: {final_df[score_columns[0].replace('_', '_combined_')].min():.2f} - {final_df[score_columns[0].replace('_', '_combined_')].max():.2f}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ⚠️  subway_score_by_tract.csv not found - using taxi scores only\")\n",
    "        print(f\"   📝 Create subway scores first: run MTA subway processing\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Error loading subway scores: {e}\")\n",
    "        print(f\"   📝 Continuing with taxi-only scores\")\n",
    "    \n",
    "    # STEP 6: Save and analyze results\n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_df.to_csv('foot_traffic_ml_optimized.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n💾 SAVED: foot_traffic_ml_optimized.csv\")\n",
    "    print(f\"📊 {len(final_df)} zones × {len(final_df.columns)} columns\")\n",
    "    \n",
    "    # STEP 7: Analyze for ML readiness\n",
    "    print(f\"\\n🤖 ML READINESS ANALYSIS:\")\n",
    "    \n",
    "    # Analyze taxi-only scores\n",
    "    taxi_score_cols = [col for col in final_df.columns \n",
    "                      if col.endswith(tuple(str(y) for y in years)) \n",
    "                      and 'combined' not in col \n",
    "                      and col != 'subway_score']\n",
    "    \n",
    "    if len(taxi_score_cols) > 0:\n",
    "        taxi_scores = final_df[taxi_score_cols].values.flatten()\n",
    "        print(f\"   🚕 Taxi scores: {taxi_scores.min():.1f} - {taxi_scores.max():.1f} (mean: {taxi_scores.mean():.1f})\")\n",
    "    \n",
    "    # Analyze combined scores if available\n",
    "    combined_score_cols = [col for col in final_df.columns if 'combined' in col]\n",
    "    if len(combined_score_cols) > 0:\n",
    "        combined_scores = final_df[combined_score_cols].values.flatten()\n",
    "        print(f\"   🚇 Combined scores: {combined_scores.min():.1f} - {combined_scores.max():.1f} (mean: {combined_scores.mean():.1f})\")\n",
    "        \n",
    "        # Show the effect of subway combination\n",
    "        if len(taxi_score_cols) > 0:\n",
    "            print(f\"   📈 Subway effect: Mean combined vs taxi = {combined_scores.mean():.1f} vs {taxi_scores.mean():.1f}\")\n",
    "    \n",
    "    # Check temporal patterns (this is what was broken before!)\n",
    "    print(f\"\\n⏰ TEMPORAL PATTERN EXAMPLE (Zone 161 - highest activity):\")\n",
    "    zone_161 = final_df[final_df['GEOID'] == 161].iloc[0]\n",
    "    \n",
    "    for period in ['morning', 'afternoon', 'evening']:\n",
    "        # Show taxi scores\n",
    "        taxi_cols = [f'{period}_{year}' for year in years if f'{period}_{year}' in final_df.columns]\n",
    "        if taxi_cols:\n",
    "            taxi_scores = [zone_161[col] for col in taxi_cols]\n",
    "            frontend_scores = [s * 10 for s in taxi_scores]  # What frontend will show\n",
    "            trend = \"↗️\" if taxi_scores[-1] > taxi_scores[0] else \"↘️\" if taxi_scores[-1] < taxi_scores[0] else \"➡️\"\n",
    "            print(f\"   🚕 {period:10}: {' → '.join([f'{s:.1f}' for s in taxi_scores])} (frontend: {' → '.join([f'{s:.0f}' for s in frontend_scores])}) {trend}\")\n",
    "        \n",
    "        # Show combined scores if available\n",
    "        combined_cols = [f'{period}_combined_{year}' for year in years if f'{period}_combined_{year}' in final_df.columns]\n",
    "        if combined_cols:\n",
    "            combined_scores = [zone_161[col] for col in combined_cols]\n",
    "            frontend_combined = [s * 10 for s in combined_scores]\n",
    "            trend = \"↗️\" if combined_scores[-1] > combined_scores[0] else \"↘️\" if combined_scores[-1] < combined_scores[0] else \"➡️\"\n",
    "            print(f\"   🚇 {period:10}: {' → '.join([f'{s:.1f}' for s in combined_scores])} (frontend: {' → '.join([f'{s:.0f}' for s in frontend_combined])}) {trend}\")\n",
    "    \n",
    "    print(f\"   ↑ These trends are now MEANINGFUL for ML!\")\n",
    "    \n",
    "    # Show zones suitable for different activity levels\n",
    "    print(f\"\\n🎯 ZONE ACTIVITY DISTRIBUTION:\")\n",
    "    avg_cols = [f'average_{year}' for year in years]\n",
    "    final_df['overall_avg'] = final_df[avg_cols].mean(axis=1)\n",
    "    \n",
    "    activity_levels = {\n",
    "        'Very High (8-10)': (final_df['overall_avg'] >= 8).sum(),\n",
    "        'High (6-8)': ((final_df['overall_avg'] >= 6) & (final_df['overall_avg'] < 8)).sum(),\n",
    "        'Medium (4-6)': ((final_df['overall_avg'] >= 4) & (final_df['overall_avg'] < 6)).sum(),\n",
    "        'Low (2-4)': ((final_df['overall_avg'] >= 2) & (final_df['overall_avg'] < 4)).sum(),\n",
    "        'Very Low (0-2)': (final_df['overall_avg'] < 2).sum()\n",
    "    }\n",
    "    \n",
    "    for level, count in activity_levels.items():\n",
    "        print(f\"   {level}: {count} zones\")\n",
    "    \n",
    "    # ML-specific recommendations\n",
    "    print(f\"\\n✅ ML OPTIMIZATION RESULTS:\")\n",
    "    print(f\"   🎯 Global normalization preserves temporal relationships\")\n",
    "    print(f\"   📈 Year-over-year trends are now meaningful\") \n",
    "    print(f\"   ⏰ Time period comparisons work across years\")\n",
    "    print(f\"   🔢 0-10 scale (frontend ×10 = your chart values)\")\n",
    "    print(f\"   🚇 Combined taxi (65%) + subway (35%) scores when available\")\n",
    "    print(f\"   📅 4-year dataset (2020-2023) for robust trend learning\")\n",
    "    print(f\"   🤖 Ready for time series ML models\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def create_trend_features_for_ml(df):\n",
    "    \"\"\"\n",
    "    Add ML-specific trend features to the optimized dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔧 ADDING ML TREND FEATURES...\")\n",
    "    \n",
    "    years = [2020, 2021, 2022, 2023]\n",
    "    periods = ['morning', 'afternoon', 'evening', 'night', 'average']\n",
    "    \n",
    "    # Add trend features\n",
    "    for period in periods:\n",
    "        period_cols = [f'{period}_{year}' for year in years if f'{period}_{year}' in df.columns]\n",
    "        \n",
    "        if len(period_cols) >= 2:\n",
    "            # Linear trend (slope)\n",
    "            trends = []\n",
    "            for idx, row in df.iterrows():\n",
    "                values = [row[col] for col in period_cols]\n",
    "                x = np.array(range(len(values)))\n",
    "                trend = np.polyfit(x, values, 1)[0] if len(values) >= 2 else 0\n",
    "                trends.append(trend)\n",
    "            \n",
    "            df[f'{period}_trend_slope'] = trends\n",
    "            \n",
    "            # Year-over-year growth rate\n",
    "            if len(period_cols) >= 2:\n",
    "                df[f'{period}_growth_rate'] = ((df[period_cols[-1]] - df[period_cols[0]]) / (df[period_cols[0]] + 1)) * 100\n",
    "    \n",
    "    # Add seasonal patterns\n",
    "    df['prefers_morning'] = df[[f'morning_{y}' for y in years]].mean(axis=1)\n",
    "    df['prefers_afternoon'] = df[[f'afternoon_{y}' for y in years]].mean(axis=1)\n",
    "    df['prefers_evening'] = df[[f'evening_{y}' for y in years]].mean(axis=1)\n",
    "    df['prefers_night'] = df[[f'night_{y}' for y in years]].mean(axis=1)\n",
    "    \n",
    "    # Find peak period for each zone\n",
    "    time_cols = ['prefers_morning', 'prefers_afternoon', 'prefers_evening', 'prefers_night']\n",
    "    df['peak_period'] = df[time_cols].idxmax(axis=1).str.replace('prefers_', '')\n",
    "    \n",
    "    # Save enhanced version\n",
    "    df.to_csv('foot_traffic_ml_ready.csv', index=False)\n",
    "    \n",
    "    print(f\"   ✅ Added trend slopes, growth rates, seasonal preferences (4-year data)\")\n",
    "    print(f\"   💾 Saved: foot_traffic_ml_ready.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 RUNNING ML-OPTIMIZED PROCESSOR\")\n",
    "    print(\"Goal: 0-10 decimal scores + taxi/subway combination + ML-ready temporal patterns\")\n",
    "    print(\"Data: 2020-2023 (4 years) | Combination: 65% taxi + 35% subway\")\n",
    "    print(\"📂 Run from: foot_traffic_score/ directory\")\n",
    "    print()\n",
    "    \n",
    "    # Process with global normalization\n",
    "    optimized_df = ml_optimized_foot_traffic_processor()\n",
    "    \n",
    "    # Add ML features\n",
    "    ml_ready_df = create_trend_features_for_ml(optimized_df)\n",
    "    \n",
    "    print(f\"\\n🎉 COMPLETE!\")\n",
    "    print(f\"✅ foot_traffic_ml_optimized.csv - All scores (taxi + combined)\")\n",
    "    print(f\"✅ foot_traffic_taxi_only.csv - Taxi scores only\") \n",
    "    print(f\"✅ foot_traffic_combined_only.csv - Combined scores only (if subway available)\")\n",
    "    print(f\"✅ foot_traffic_ml_ready.csv - Enhanced with ML features\") \n",
    "    print(f\"📂 All files saved to: foot_traffic_score/ directory\")\n",
    "    print(f\"📊 Frontend: multiply by 10 for display (9.3 → 93)\")\n",
    "    print(f\"🚇 Combination: 65% taxi + 35% subway (following your project pattern)\")\n",
    "    print(f\"🤖 Optimized for time series ML models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 FIXED Foot Traffic ML Pipeline\n",
      "==================================================\n",
      "\n",
      "📂 Step 1: Loading Data...\n",
      "✅ Loaded: foot_traffic_ml_ready.csv\n",
      "📊 Original dataset shape: (49, 38)\n",
      "📊 Columns: ['id', 'GEOID', 'morning_2020', 'afternoon_2020', 'evening_2020', 'night_2020', 'average_2020', 'morning_2021', 'afternoon_2021', 'evening_2021', 'night_2021', 'average_2021', 'morning_2022', 'afternoon_2022', 'evening_2022', 'night_2022', 'average_2022', 'morning_2023', 'afternoon_2023', 'evening_2023', 'night_2023', 'average_2023', 'overall_avg', 'morning_trend_slope', 'morning_growth_rate', 'afternoon_trend_slope', 'afternoon_growth_rate', 'evening_trend_slope', 'evening_growth_rate', 'night_trend_slope', 'night_growth_rate', 'average_trend_slope', 'average_growth_rate', 'prefers_morning', 'prefers_afternoon', 'prefers_evening', 'prefers_night', 'peak_period']\n",
      "📊 Sample data:\n",
      "   id  GEOID  morning_2020  afternoon_2020  evening_2020  night_2020  \\\n",
      "0   1      4      0.099753        0.227694      0.276751    0.101110   \n",
      "1   2     12      0.027886        0.023444      0.008507    0.003342   \n",
      "2   3     13      0.382973        0.430579      0.346035    0.055737   \n",
      "3   4     14      0.009537        0.023296      0.035311    0.013701   \n",
      "4   5     24      0.133564        0.224228      0.180157    0.038505   \n",
      "\n",
      "   average_2020  morning_2021  afternoon_2021  evening_2021  ...  \\\n",
      "0      0.707390      0.108697        0.288499      0.330167  ...   \n",
      "1      0.065262      0.042847        0.042207      0.011456  ...   \n",
      "2      1.217405      0.390345        0.563092      0.411087  ...   \n",
      "3      0.083927      0.009699        0.025369      0.031224  ...   \n",
      "4      0.578536      0.147271        0.284326      0.227077  ...   \n",
      "\n",
      "   evening_growth_rate  night_trend_slope  night_growth_rate  \\\n",
      "0             9.283378           0.031144           7.423110   \n",
      "1             0.346001          -0.000497          -0.176327   \n",
      "2            16.357387           0.018373           4.632167   \n",
      "3             0.497179           0.001653           0.403037   \n",
      "4             5.076007           0.011201           2.555532   \n",
      "\n",
      "   average_trend_slope  average_growth_rate  prefers_morning  \\\n",
      "0             0.074599            11.672022         0.101945   \n",
      "1             0.036529             9.258683         0.060857   \n",
      "2             0.283753            33.012819         0.507369   \n",
      "3             0.002320             0.352174         0.008350   \n",
      "4             0.031105             4.424485         0.140608   \n",
      "\n",
      "   prefers_afternoon  prefers_evening  prefers_night  peak_period  \n",
      "0           0.256269         0.357428       0.136271      evening  \n",
      "1           0.048265         0.011418       0.002356      morning  \n",
      "2           0.604435         0.472141       0.080722    afternoon  \n",
      "3           0.022959         0.037713       0.016936      evening  \n",
      "4           0.250834         0.234979       0.058719    afternoon  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "\n",
      "🔄 Step 2: Reshaping Data for ML...\n",
      "📊 Found time period columns: 16\n",
      "📊 Examples: ['morning_2020', 'afternoon_2020', 'evening_2020', 'night_2020', 'morning_2021']\n",
      "📊 Created 784 ML training rows\n",
      "📊 ML DataFrame shape: (784, 19)\n",
      "📊 Unique GEOIDs: 49\n",
      "📊 Years: [np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023)]\n",
      "📊 Time periods: ['afternoon', 'evening', 'morning', 'night']\n",
      "📊 Score range: 0.000 to 4.008\n",
      "\n",
      "🔧 Step 3: Feature Engineering...\n",
      "✅ Added 15 trend features\n",
      "📋 Final features (18): ['geoid_encoded', 'period_encoded', 'year_normalized', 'morning_trend_slope', 'morning_growth_rate', 'afternoon_trend_slope', 'afternoon_growth_rate', 'evening_trend_slope', 'evening_growth_rate', 'night_trend_slope', 'night_growth_rate', 'average_trend_slope', 'average_growth_rate', 'prefers_morning', 'prefers_afternoon', 'prefers_evening', 'prefers_night', 'overall_avg']\n",
      "📊 Final training data: X=(784, 18), y=(784,)\n",
      "\n",
      "🤖 Step 4: Testing Models...\n",
      "📊 Training set: (627, 18)\n",
      "📊 Test set: (157, 18)\n",
      "\n",
      "--- Training RandomForest ---\n",
      "RandomForest → R²: 0.9585, MAE: 0.0810, RMSE: 0.1307\n",
      "\n",
      "--- Training HistGradientBoosting ---\n",
      "HistGradientBoosting → R²: 0.9534, MAE: 0.0700, RMSE: 0.1385\n",
      "\n",
      "--- Training GradientBoosting ---\n",
      "GradientBoosting → R²: 0.9287, MAE: 0.1107, RMSE: 0.1713\n",
      "\n",
      "--- Training LinearRegression ---\n",
      "LinearRegression → R²: 0.7231, MAE: 0.2608, RMSE: 0.3375\n",
      "\n",
      "--- Training Ridge ---\n",
      "Ridge → R²: 0.7299, MAE: 0.2582, RMSE: 0.3333\n",
      "\n",
      "--- Training DecisionTree ---\n",
      "DecisionTree → R²: 0.9312, MAE: 0.0921, RMSE: 0.1682\n",
      "\n",
      "--- Training KNN ---\n",
      "KNN → R²: 0.8593, MAE: 0.1561, RMSE: 0.2406\n",
      "\n",
      "🏆 Step 5: Results Summary\n",
      "============================================================\n",
      "RandomForest         | R²: 0.9585 | MAE: 0.0810 | RMSE: 0.1307\n",
      "HistGradientBoosting | R²: 0.9534 | MAE: 0.0700 | RMSE: 0.1385\n",
      "DecisionTree         | R²: 0.9312 | MAE: 0.0921 | RMSE: 0.1682\n",
      "GradientBoosting     | R²: 0.9287 | MAE: 0.1107 | RMSE: 0.1713\n",
      "KNN                  | R²: 0.8593 | MAE: 0.1561 | RMSE: 0.2406\n",
      "Ridge                | R²: 0.7299 | MAE: 0.2582 | RMSE: 0.3333\n",
      "LinearRegression     | R²: 0.7231 | MAE: 0.2608 | RMSE: 0.3375\n",
      "\n",
      "🥇 Best Model: RandomForest (R² = 0.9585)\n",
      "\n",
      "🔮 Step 6: Generating Future Predictions...\n",
      "📍 Predicting for 49 GEOIDs, 4 time periods, 3 years\n",
      "\n",
      "📊 Step 7: Formatting Output...\n",
      "📊 Final output shape: (49, 16)\n",
      "📊 Prediction columns: ['afternoon_pred_2025', 'afternoon_pred_2026', 'afternoon_pred_2027', 'evening_pred_2025', 'evening_pred_2026', 'evening_pred_2027', 'morning_pred_2025', 'morning_pred_2026', 'morning_pred_2027', 'night_pred_2025', 'night_pred_2026', 'night_pred_2027', 'average_pred_2025', 'average_pred_2026', 'average_pred_2027']\n",
      "\n",
      "📊 Step 8: Creating Final Database Format...\n",
      "   Adding predictions for 2024...\n",
      "   Adding predictions for 2025...\n",
      "   Adding predictions for 2026...\n",
      "   Adding predictions for 2027...\n",
      "🔧 Applied scale factor: 2.605\n",
      "✅ Saved: tract_foot_traffic_trends_rows 1.csv\n",
      "📊 Final clean format: (49, 34)\n",
      "📊 Columns: 34 (target: 34)\n",
      "\n",
      "📋 Sample of clean output:\n",
      "   id  GEOID  morning_2024  morning_pred_2025  afternoon_2024  \\\n",
      "0   1      4      0.292504           0.292504        0.687152   \n",
      "1   2     12      0.183956           0.183956        0.146300   \n",
      "2   3     13      1.506519           1.506519        1.732783   \n",
      "3   4     14      0.025993           0.025993        0.058487   \n",
      "4   5     24      0.376523           0.376523        0.714283   \n",
      "\n",
      "   afternoon_pred_2025  evening_2024  evening_pred_2025  \n",
      "0             0.687152      0.917815           0.917815  \n",
      "1             0.146300      0.114348           0.114348  \n",
      "2             1.732783      1.460021           1.460021  \n",
      "3             0.058487      0.100055           0.100055  \n",
      "4             0.714283      0.746441           0.746441  \n",
      "\n",
      "🎉 FIXED Pipeline Complete!\n",
      "🏆 Best model: RandomForest (R² = 0.9585)\n",
      "📊 Output ready for database!\n",
      "\n",
      "📋 Sample of clean output:\n",
      "   id  GEOID  morning_2024  morning_pred_2025  afternoon_2024  \\\n",
      "0   1      4      0.292504           0.292504        0.687152   \n",
      "1   2     12      0.183956           0.183956        0.146300   \n",
      "2   3     13      1.506519           1.506519        1.732783   \n",
      "3   4     14      0.025993           0.025993        0.058487   \n",
      "4   5     24      0.376523           0.376523        0.714283   \n",
      "\n",
      "   afternoon_pred_2025  evening_2024  evening_pred_2025  \n",
      "0             0.687152      0.917815           0.917815  \n",
      "1             0.146300      0.114348           0.114348  \n",
      "2             1.732783      1.460021           1.460021  \n",
      "3             0.058487      0.100055           0.100055  \n",
      "4             0.714283      0.746441           0.746441  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def fixed_foot_traffic_ml_pipeline():\n",
    "    \"\"\"\n",
    "    FIXED ML pipeline that works with your actual data structure\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 FIXED Foot Traffic ML Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Load data with better error handling\n",
    "    print(\"\\n📂 Step 1: Loading Data...\")\n",
    "    \n",
    "    df = None\n",
    "    use_ml_features = False\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv('foot_traffic_ml_ready.csv')\n",
    "        print(f\"✅ Loaded: foot_traffic_ml_ready.csv\")\n",
    "        use_ml_features = True\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            df = pd.read_csv('foot_traffic_ml_optimized.csv')\n",
    "            print(f\"✅ Loaded: foot_traffic_ml_optimized.csv\")\n",
    "            use_ml_features = False\n",
    "        except FileNotFoundError:\n",
    "            print(\"❌ Could not find data files!\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"📊 Original dataset shape: {df.shape}\")\n",
    "    print(f\"📊 Columns: {df.columns.tolist()}\")\n",
    "    print(f\"📊 Sample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Step 2: Reshape data properly\n",
    "    print(f\"\\n🔄 Step 2: Reshaping Data for ML...\")\n",
    "    \n",
    "    # Identify time period columns automatically\n",
    "    time_columns = []\n",
    "    for col in df.columns:\n",
    "        if any(period in col for period in ['morning', 'afternoon', 'evening', 'night']) and any(str(year) in col for year in [2020, 2021, 2022, 2023]):\n",
    "            time_columns.append(col)\n",
    "    \n",
    "    print(f\"📊 Found time period columns: {len(time_columns)}\")\n",
    "    print(f\"📊 Examples: {time_columns[:5]}\")\n",
    "    \n",
    "    # Reshape to long format\n",
    "    reshaped_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        geoid = row['GEOID']\n",
    "        \n",
    "        for col in time_columns:\n",
    "            if pd.notna(row[col]):  # Only process non-NaN values\n",
    "                # Parse column name to extract period and year\n",
    "                parts = col.split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    period = parts[0]\n",
    "                    year = int(parts[1])\n",
    "                    score = row[col]\n",
    "                    \n",
    "                    # Create ML row\n",
    "                    ml_row = {\n",
    "                        'GEOID': geoid,\n",
    "                        'year': year,\n",
    "                        'time_period': period,\n",
    "                        'foot_traffic_score': score\n",
    "                    }\n",
    "                    \n",
    "                    # Add trend features if available\n",
    "                    if use_ml_features:\n",
    "                        trend_cols = [c for c in df.columns if 'trend' in c or 'growth' in c or 'prefers' in c]\n",
    "                        for trend_col in trend_cols:\n",
    "                            if trend_col in df.columns:\n",
    "                                ml_row[trend_col] = row[trend_col]\n",
    "                        \n",
    "                        # Add overall average if available\n",
    "                        if 'overall_avg' in df.columns:\n",
    "                            ml_row['overall_avg'] = row['overall_avg']\n",
    "                    \n",
    "                    reshaped_data.append(ml_row)\n",
    "    \n",
    "    print(f\"📊 Created {len(reshaped_data)} ML training rows\")\n",
    "    \n",
    "    if len(reshaped_data) == 0:\n",
    "        print(\"❌ No valid training data created!\")\n",
    "        print(\"🔍 Debug info:\")\n",
    "        print(f\"   Time columns found: {time_columns}\")\n",
    "        print(f\"   Sample values: {[df[col].iloc[0] if col in df.columns else 'N/A' for col in time_columns[:3]]}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    ml_df = pd.DataFrame(reshaped_data)\n",
    "    \n",
    "    print(f\"📊 ML DataFrame shape: {ml_df.shape}\")\n",
    "    print(f\"📊 Unique GEOIDs: {ml_df['GEOID'].nunique()}\")\n",
    "    print(f\"📊 Years: {sorted(ml_df['year'].unique())}\")\n",
    "    print(f\"📊 Time periods: {sorted(ml_df['time_period'].unique())}\")\n",
    "    print(f\"📊 Score range: {ml_df['foot_traffic_score'].min():.3f} to {ml_df['foot_traffic_score'].max():.3f}\")\n",
    "    \n",
    "    # Step 3: Feature engineering\n",
    "    print(f\"\\n🔧 Step 3: Feature Engineering...\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_geoid = LabelEncoder()\n",
    "    le_period = LabelEncoder()\n",
    "    \n",
    "    ml_df['geoid_encoded'] = le_geoid.fit_transform(ml_df['GEOID'])\n",
    "    ml_df['period_encoded'] = le_period.fit_transform(ml_df['time_period'])\n",
    "    \n",
    "    # Create time-based features\n",
    "    ml_df['year_normalized'] = (ml_df['year'] - ml_df['year'].min()) / (ml_df['year'].max() - ml_df['year'].min())\n",
    "    \n",
    "    # Basic features that always exist\n",
    "    base_features = ['geoid_encoded', 'period_encoded', 'year_normalized']\n",
    "    \n",
    "    # Add trend features if available\n",
    "    feature_columns = base_features.copy()\n",
    "    if use_ml_features:\n",
    "        trend_features = [col for col in ml_df.columns if col not in ['GEOID', 'year', 'time_period', 'foot_traffic_score'] + base_features]\n",
    "        feature_columns.extend(trend_features)\n",
    "        print(f\"✅ Added {len(trend_features)} trend features\")\n",
    "    \n",
    "    target_column = 'foot_traffic_score'\n",
    "    \n",
    "    print(f\"📋 Final features ({len(feature_columns)}): {feature_columns}\")\n",
    "    \n",
    "    # Prepare final data\n",
    "    X = ml_df[feature_columns].copy()\n",
    "    y = ml_df[target_column].copy()\n",
    "    \n",
    "    # Handle any remaining NaN values\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    print(f\"📊 Final training data: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    # Step 4: Model testing\n",
    "    print(f\"\\n🤖 Step 4: Testing Models...\")\n",
    "    \n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        \"HistGradientBoosting\": HistGradientBoostingRegressor(random_state=42),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(random_state=42, n_estimators=100),\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(alpha=1.0),\n",
    "        \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "        \"KNN\": KNeighborsRegressor(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"📊 Training set: {X_train.shape}\")\n",
    "    print(f\"📊 Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Test models\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_model_obj = None\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            print(f\"\\n--- Training {name} ---\")\n",
    "            \n",
    "            # Fit model\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Metrics\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'R²': r2,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse\n",
    "            })\n",
    "            \n",
    "            print(f\"{name} → R²: {r2:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "            \n",
    "            # Track best model\n",
    "            if r2 > best_score:\n",
    "                best_score = r2\n",
    "                best_model = name\n",
    "                best_model_obj = model\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {name} failed: {e}\")\n",
    "    \n",
    "    # Step 5: Results\n",
    "    print(f\"\\n🏆 Step 5: Results Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('R²', ascending=False)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"{row['Model'].ljust(20)} | R²: {row['R²']:.4f} | MAE: {row['MAE']:.4f} | RMSE: {row['RMSE']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🥇 Best Model: {best_model} (R² = {best_score:.4f})\")\n",
    "    \n",
    "    # Step 6: Generate predictions\n",
    "    print(f\"\\n🔮 Step 6: Generating Future Predictions...\")\n",
    "    \n",
    "    future_years = [2025, 2026, 2027]\n",
    "    unique_geoids = df['GEOID'].unique()\n",
    "    unique_periods = ml_df['time_period'].unique()\n",
    "    \n",
    "    print(f\"📍 Predicting for {len(unique_geoids)} GEOIDs, {len(unique_periods)} time periods, {len(future_years)} years\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    future_predictions = []\n",
    "    \n",
    "    for year in future_years:\n",
    "        for geoid in unique_geoids:\n",
    "            for period in unique_periods:\n",
    "                # Create prediction row\n",
    "                pred_row = {\n",
    "                    'geoid_encoded': le_geoid.transform([geoid])[0],\n",
    "                    'period_encoded': le_period.transform([period])[0],\n",
    "                    'year_normalized': (year - ml_df['year'].min()) / (ml_df['year'].max() - ml_df['year'].min())\n",
    "                }\n",
    "                \n",
    "                # Add trend features if available\n",
    "                if use_ml_features:\n",
    "                    geoid_data = df[df['GEOID'] == geoid].iloc[0]\n",
    "                    for feat in trend_features:\n",
    "                        if feat in geoid_data:\n",
    "                            pred_row[feat] = geoid_data[feat]\n",
    "                        else:\n",
    "                            pred_row[feat] = 0\n",
    "                \n",
    "                # Make prediction\n",
    "                pred_features = pd.DataFrame([pred_row])[feature_columns]\n",
    "                pred_features = pred_features.fillna(pred_features.median())\n",
    "                pred_scaled = scaler.transform(pred_features)\n",
    "                \n",
    "                prediction = best_model_obj.predict(pred_scaled)[0]\n",
    "                \n",
    "                future_predictions.append({\n",
    "                    'GEOID': geoid,\n",
    "                    'year': year,\n",
    "                    'time_period': period,\n",
    "                    'predicted_score': prediction\n",
    "                })\n",
    "    \n",
    "    # Step 7: Format output\n",
    "    print(f\"\\n📊 Step 7: Formatting Output...\")\n",
    "    \n",
    "    pred_df = pd.DataFrame(future_predictions)\n",
    "    \n",
    "    # Pivot to database format\n",
    "    pivot_df = pred_df.pivot_table(\n",
    "        index='GEOID',\n",
    "        columns=['time_period', 'year'],\n",
    "        values='predicted_score',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Flatten column names\n",
    "    pivot_df.columns = [f'{period}_pred_{year}' for period, year in pivot_df.columns]\n",
    "    pivot_df = pivot_df.reset_index()\n",
    "    \n",
    "    # Add average predictions\n",
    "    for year in future_years:\n",
    "        year_cols = [col for col in pivot_df.columns if f'pred_{year}' in col and 'average' not in col]\n",
    "        if year_cols:\n",
    "            pivot_df[f'average_pred_{year}'] = pivot_df[year_cols].mean(axis=1)\n",
    "    \n",
    "    # Ensure 0-10 range\n",
    "    pred_cols = [col for col in pivot_df.columns if 'pred_' in col]\n",
    "    for col in pred_cols:\n",
    "        pivot_df[col] = np.clip(pivot_df[col], 0, 10)\n",
    "    \n",
    "    print(f\"📊 Final output shape: {pivot_df.shape}\")\n",
    "    print(f\"📊 Prediction columns: {[col for col in pivot_df.columns if 'pred_' in col]}\")\n",
    "    \n",
    "    # Step 8: Create final format matching target structure\n",
    "    print(f\"\\n📊 Step 8: Creating Final Database Format...\")\n",
    "    \n",
    "    # Start fresh with clean format\n",
    "    unique_geoids = df['GEOID'].unique()\n",
    "    clean_df = pd.DataFrame({'GEOID': unique_geoids})\n",
    "    clean_df['id'] = range(1, len(clean_df) + 1)\n",
    "    \n",
    "    # Add historical data (2020-2023) from original data\n",
    "    time_periods_clean = ['morning', 'afternoon', 'evening']  # Match target format\n",
    "    historical_years = [2020, 2021, 2022, 2023]  # Only years you actually have\n",
    "    \n",
    "    for period in time_periods_clean:\n",
    "        for year in historical_years:\n",
    "            col_name = f'{period}_{year}'\n",
    "            if col_name in df.columns:\n",
    "                clean_df = clean_df.merge(\n",
    "                    df[['GEOID', col_name]], \n",
    "                    on='GEOID', \n",
    "                    how='left'\n",
    "                )\n",
    "            else:\n",
    "                # Fill missing years with zeros or interpolated values\n",
    "                clean_df[col_name] = 0.0\n",
    "    \n",
    "    # Add average columns for historical years\n",
    "    for year in historical_years:\n",
    "        year_cols = [f'{period}_{year}' for period in time_periods_clean if f'{period}_{year}' in clean_df.columns]\n",
    "        if year_cols:\n",
    "            clean_df[f'average_{year}'] = clean_df[year_cols].mean(axis=1)\n",
    "        else:\n",
    "            clean_df[f'average_{year}'] = 0.0\n",
    "    \n",
    "    # Add predictions from ML model (2024-2027)\n",
    "    prediction_years = [2024, 2025, 2026, 2027] \n",
    "    \n",
    "    # Generate predictions for each year\n",
    "    for year in prediction_years:\n",
    "        print(f\"   Adding predictions for {year}...\")\n",
    "        \n",
    "        for period in time_periods_clean:\n",
    "            period_predictions = []\n",
    "            \n",
    "            for geoid in unique_geoids:\n",
    "                # Create prediction features\n",
    "                pred_row = {\n",
    "                    'geoid_encoded': le_geoid.transform([geoid])[0],\n",
    "                    'period_encoded': le_period.transform([period])[0],\n",
    "                    'year_normalized': (year - ml_df['year'].min()) / (ml_df['year'].max() - ml_df['year'].min())\n",
    "                }\n",
    "                \n",
    "                # Add trend features if available\n",
    "                if use_ml_features:\n",
    "                    geoid_data = df[df['GEOID'] == geoid].iloc[0]\n",
    "                    for feat in trend_features:\n",
    "                        if feat in geoid_data:\n",
    "                            pred_row[feat] = geoid_data[feat]\n",
    "                        else:\n",
    "                            pred_row[feat] = 0\n",
    "                \n",
    "                # Make prediction\n",
    "                pred_features = pd.DataFrame([pred_row])[feature_columns]\n",
    "                pred_features = pred_features.fillna(pred_features.median())\n",
    "                pred_scaled = scaler.transform(pred_features)\n",
    "                \n",
    "                prediction = best_model_obj.predict(pred_scaled)[0]\n",
    "                period_predictions.append(prediction)\n",
    "            \n",
    "            # Add to clean dataframe\n",
    "            if year == 2024:\n",
    "                col_name = f'{period}_{year}'\n",
    "            else:\n",
    "                col_name = f'{period}_pred_{year}'\n",
    "            \n",
    "            clean_df[col_name] = period_predictions\n",
    "    \n",
    "    # Add average predictions\n",
    "    for year in prediction_years:\n",
    "        if year == 2024:\n",
    "            year_cols = [f'{period}_{year}' for period in time_periods_clean]\n",
    "            clean_df[f'average_{year}'] = clean_df[year_cols].mean(axis=1)\n",
    "        else:\n",
    "            year_cols = [f'{period}_pred_{year}' for period in time_periods_clean]\n",
    "            clean_df[f'average_pred_{year}'] = clean_df[year_cols].mean(axis=1)\n",
    "    \n",
    "    # Scale predictions to 0-10 range\n",
    "    pred_cols = []\n",
    "    for year in prediction_years:\n",
    "        for period in time_periods_clean:\n",
    "            if year == 2024:\n",
    "                pred_cols.append(f'{period}_{year}')\n",
    "            else:\n",
    "                pred_cols.append(f'{period}_pred_{year}')\n",
    "        \n",
    "        if year == 2024:\n",
    "            pred_cols.append(f'average_{year}')\n",
    "        else:\n",
    "            pred_cols.append(f'average_pred_{year}')\n",
    "    \n",
    "    # Apply scaling to predictions only\n",
    "    if pred_cols:\n",
    "        pred_data = clean_df[pred_cols]\n",
    "        current_max = pred_data.max().max()\n",
    "        if current_max > 0:\n",
    "            scale_factor = 10.0 / current_max\n",
    "            clean_df[pred_cols] = clean_df[pred_cols] * scale_factor\n",
    "            print(f\"🔧 Applied scale factor: {scale_factor:.3f}\")\n",
    "    \n",
    "    # Ensure exact column order matching target format\n",
    "    expected_columns = ['id', 'GEOID']\n",
    "    \n",
    "    # Add time period columns in order (2020-2023 + 2024-2027)\n",
    "    all_years = [2020, 2021, 2022, 2023, 2024, 'pred_2025', 'pred_2026', 'pred_2027']\n",
    "    \n",
    "    for period in time_periods_clean:\n",
    "        for year in all_years:\n",
    "            if year == 2024:\n",
    "                col_name = f'{period}_{year}'\n",
    "            elif isinstance(year, str):  # pred_YYYY\n",
    "                col_name = f'{period}_{year}'\n",
    "            else:\n",
    "                col_name = f'{period}_{year}'\n",
    "            \n",
    "            if col_name in clean_df.columns:\n",
    "                expected_columns.append(col_name)\n",
    "    \n",
    "    # Add average columns\n",
    "    for year in all_years:\n",
    "        if year == 2024:\n",
    "            col_name = f'average_{year}'\n",
    "        elif isinstance(year, str):  # pred_YYYY\n",
    "            col_name = f'average_{year}'\n",
    "        else:\n",
    "            col_name = f'average_{year}'\n",
    "        \n",
    "        if col_name in clean_df.columns:\n",
    "            expected_columns.append(col_name)\n",
    "    \n",
    "    # Reorder and clean up\n",
    "    for col in expected_columns:\n",
    "        if col not in clean_df.columns:\n",
    "            clean_df[col] = 0.0\n",
    "    \n",
    "    final_clean_df = clean_df[expected_columns]\n",
    "    \n",
    "    # Save to exact target filename\n",
    "    output_filename = 'tract_foot_traffic_trends_rows 1.csv'\n",
    "    final_clean_df.to_csv(output_filename, index=False)\n",
    "    print(f\"✅ Saved: {output_filename}\")\n",
    "    \n",
    "    print(f\"📊 Final clean format: {final_clean_df.shape}\")\n",
    "    print(f\"📊 Columns: {len(final_clean_df.columns)} (target: 34)\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\n📋 Sample of clean output:\")\n",
    "    sample_cols = ['id', 'GEOID'] + [col for col in final_clean_df.columns if '2024' in col or 'pred_2025' in col][:6]\n",
    "    print(final_clean_df[sample_cols].head())\n",
    "    \n",
    "    print(f\"\\n🎉 FIXED Pipeline Complete!\")\n",
    "    print(f\"🏆 Best model: {best_model} (R² = {best_score:.4f})\")\n",
    "    print(f\"📊 Output ready for database!\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\n📋 Sample of clean output:\")\n",
    "    sample_cols = ['id', 'GEOID'] + [col for col in final_clean_df.columns if 'pred_2025' in col or '2024' in col][:6]\n",
    "    print(final_clean_df[sample_cols].head())\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_score': best_score,\n",
    "        'predictions': final_clean_df,\n",
    "        'encoders': {'geoid': le_geoid, 'period': le_period},\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "# Run the fixed pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    results = fixed_foot_traffic_ml_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp47350py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
