{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ RUNNING ML-OPTIMIZED PROCESSOR\n",
      "Goal: 0-10 decimal scores + taxi/subway combination + ML-ready temporal patterns\n",
      "Data: 2020-2023 (4 years) | Combination: 65% taxi + 35% subway\n",
      "üìÇ Run from: foot_traffic_score/ directory\n",
      "\n",
      "ü§ñ ML-Optimized Foot Traffic Processor\n",
      "üéØ Goal: 0-10 scale (√ó10 on frontend) + proper time series ML\n",
      "üöá Includes: Taxi (65%) + Subway (35%) combination\n",
      "üìÖ Data: 2020-2023 (4 years for robust ML training)\n",
      "======================================================================\n",
      "üìä STEP 1: Collecting ALL raw trip counts...\n",
      "   üìÖ Processing 2020... "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "def ml_optimized_foot_traffic_processor():\n",
    "    \"\"\"\n",
    "    ML-OPTIMIZED VERSION: Global normalization for time series prediction\n",
    "    INCLUDES: Taxi foot traffic + subway scores combination (65%/35% weighting)\n",
    "    \n",
    "    Key insight: ALL data (all years, all periods) normalized together\n",
    "    This preserves temporal relationships crucial for ML learning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ü§ñ ML-Optimized Foot Traffic Processor\")\n",
    "    print(\"üéØ Goal: 0-10 scale (√ó10 on frontend) + proper time series ML\")\n",
    "    print(\"üöá Includes: Taxi (65%) + Subway (35%) combination\")\n",
    "    print(\"üìÖ Data: 2020-2023 (4 years for robust ML training)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    files = {\n",
    "        2020: 'YellowTaxiYears/2020_Yellow_Taxi_Trip_Data.csv',\n",
    "        2021: 'YellowTaxiYears/2021_Yellow_Taxi_Trip_Data.csv', \n",
    "        2022: 'YellowTaxiYears/2022_Yellow_Taxi_Trip_Data.csv',\n",
    "        2023: 'YellowTaxiYears/2023_Yellow_Taxi_Trip_Data.csv'\n",
    "    }\n",
    "    \n",
    "    manhattan_zones = [4, 12, 13, 14, 24, 41, 42, 43, 45, 48, 50, 68, 74, 75, 79, 87, 88, 90, 100, 107, 113, 114, 116, 125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166, 170, 186, 230, 231, 232, 233, 234]\n",
    "    \n",
    "    # STEP 1: Collect ALL raw counts (every year, every period, every zone)\n",
    "    print(\"üìä STEP 1: Collecting ALL raw trip counts...\")\n",
    "    \n",
    "    all_raw_pickup_counts = []\n",
    "    all_raw_dropoff_counts = []\n",
    "    raw_data_store = {}\n",
    "    \n",
    "    for year, file in files.items():\n",
    "        print(f\"   üìÖ Processing {year}...\", end=\" \")\n",
    "        \n",
    "        # Load and filter data\n",
    "        df = pd.read_csv(file, usecols=['tpep_pickup_datetime', 'PULocationID', 'DOLocationID'])\n",
    "        df['hour'] = pd.to_datetime(df['tpep_pickup_datetime'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce').dt.hour\n",
    "        df = df.dropna(subset=['hour'])\n",
    "        df = df[df['PULocationID'].isin(manhattan_zones) & df['DOLocationID'].isin(manhattan_zones)]\n",
    "        \n",
    "        # Define time periods\n",
    "        periods = {\n",
    "            'morning': (df['hour'] >= 6) & (df['hour'] < 12),\n",
    "            'afternoon': (df['hour'] >= 12) & (df['hour'] < 18),\n",
    "            'evening': (df['hour'] >= 18) & (df['hour'] < 24),\n",
    "            'night': (df['hour'] >= 0) & (df['hour'] < 6)\n",
    "        }\n",
    "        \n",
    "        year_pickup_counts = []\n",
    "        year_dropoff_counts = []\n",
    "        \n",
    "        # Process each time period\n",
    "        for period_name, period_mask in periods.items():\n",
    "            period_df = df[period_mask]\n",
    "            \n",
    "            # Get raw counts\n",
    "            pickups = period_df.groupby('PULocationID').size().reindex(manhattan_zones, fill_value=0).values\n",
    "            dropoffs = period_df.groupby('DOLocationID').size().reindex(manhattan_zones, fill_value=0).values\n",
    "            \n",
    "            # Store for later use\n",
    "            raw_data_store[f'{period_name}_{year}_pickup'] = pickups\n",
    "            raw_data_store[f'{period_name}_{year}_dropoff'] = dropoffs\n",
    "            \n",
    "            # Add to global collection\n",
    "            all_raw_pickup_counts.extend(pickups)\n",
    "            all_raw_dropoff_counts.extend(dropoffs)\n",
    "            year_pickup_counts.extend(pickups)\n",
    "            year_dropoff_counts.extend(dropoffs)\n",
    "        \n",
    "        # Overall year counts\n",
    "        all_pickups = df.groupby('PULocationID').size().reindex(manhattan_zones, fill_value=0).values\n",
    "        all_dropoffs = df.groupby('DOLocationID').size().reindex(manhattan_zones, fill_value=0).values\n",
    "        \n",
    "        raw_data_store[f'average_{year}_pickup'] = all_pickups\n",
    "        raw_data_store[f'average_{year}_dropoff'] = all_dropoffs\n",
    "        \n",
    "        all_raw_pickup_counts.extend(all_pickups)\n",
    "        all_raw_dropoff_counts.extend(all_dropoffs)\n",
    "        \n",
    "        print(f\"‚úÖ {len(df):,} trips, {len(year_pickup_counts)} data points\")\n",
    "    \n",
    "    # STEP 2: Global normalization strategy\n",
    "    print(f\"\\nüîÑ STEP 2: GLOBAL normalization across all data...\")\n",
    "    \n",
    "    print(f\"   üìà Raw data stats:\")\n",
    "    pickup_array = np.array(all_raw_pickup_counts)\n",
    "    dropoff_array = np.array(all_raw_dropoff_counts)\n",
    "    \n",
    "    print(f\"      Pickups: min={pickup_array.min()}, max={pickup_array.max()}, mean={pickup_array.mean():.1f}\")\n",
    "    print(f\"      Dropoffs: min={dropoff_array.min()}, max={dropoff_array.max()}, mean={dropoff_array.mean():.1f}\")\n",
    "    \n",
    "    # Strategy: MinMaxScaler to 0-10 (frontend will multiply by 10 for display)\n",
    "    print(f\"   üéØ Applying global 0-10 normalization...\")\n",
    "    \n",
    "    pickup_scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "    dropoff_scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "    \n",
    "    # Fit on ALL data\n",
    "    pickup_scaler.fit(pickup_array.reshape(-1, 1))\n",
    "    dropoff_scaler.fit(dropoff_array.reshape(-1, 1))\n",
    "    \n",
    "    # STEP 3: Apply normalization and create foot traffic scores\n",
    "    print(f\"\\n‚ö° STEP 3: Creating foot traffic scores...\")\n",
    "    \n",
    "    results = {'id': range(1, len(manhattan_zones) + 1), 'GEOID': manhattan_zones}\n",
    "    \n",
    "    years = list(files.keys())\n",
    "    periods = ['morning', 'afternoon', 'evening', 'night']\n",
    "    \n",
    "    # Process each year and period\n",
    "    for year in years:\n",
    "        for period in periods:\n",
    "            # Get raw counts\n",
    "            pickups = raw_data_store[f'{period}_{year}_pickup']\n",
    "            dropoffs = raw_data_store[f'{period}_{year}_dropoff']\n",
    "            \n",
    "            # Apply GLOBAL normalization\n",
    "            pickup_scaled = pickup_scaler.transform(pickups.reshape(-1, 1)).flatten()\n",
    "            dropoff_scaled = dropoff_scaler.transform(dropoffs.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Calculate foot traffic score (0.7 dropoff + 0.3 pickup)\n",
    "            foot_traffic_score = 0.7 * dropoff_scaled + 0.3 * pickup_scaled\n",
    "            results[f'{period}_{year}'] = foot_traffic_score\n",
    "        \n",
    "        # Process average\n",
    "        pickups = raw_data_store[f'average_{year}_pickup']\n",
    "        dropoffs = raw_data_store[f'average_{year}_dropoff']\n",
    "        \n",
    "        pickup_scaled = pickup_scaler.transform(pickups.reshape(-1, 1)).flatten()\n",
    "        dropoff_scaled = dropoff_scaler.transform(dropoffs.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        avg_score = 0.7 * dropoff_scaled + 0.3 * pickup_scaled\n",
    "        results[f'average_{year}'] = avg_score\n",
    "    \n",
    "    # STEP 4: Spatial mapping from taxi zones to census tracts\n",
    "    print(f\"\\nüó∫Ô∏è  STEP 4: Spatial mapping taxi zones ‚Üí census tracts...\")\n",
    "    \n",
    "    try:\n",
    "        # Load taxi zones shapefile - checking multiple possible locations\n",
    "        taxi_zones_paths = [\n",
    "            \"taxi_zones/taxi_zones.shp\",  # Original path\n",
    "            \"../taxi_zones/taxi_zones.shp\",  # One level up\n",
    "            \"../../taxi_zones/taxi_zones.shp\"  # Two levels up\n",
    "        ]\n",
    "        \n",
    "        taxi_zones = None\n",
    "        for path in taxi_zones_paths:\n",
    "            try:\n",
    "                taxi_zones = gpd.read_file(path)\n",
    "                taxi_zones = taxi_zones[taxi_zones[\"borough\"] == \"Manhattan\"].copy()\n",
    "                print(f\"   ‚úÖ Loaded {len(taxi_zones)} Manhattan taxi zones from {path}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if taxi_zones is None:\n",
    "            raise FileNotFoundError(\"Taxi zones shapefile not found in any expected location\")\n",
    "        \n",
    "        # Load census tracts (your exact structure)\n",
    "        census_tracts = gpd.read_file(\"../census tract geofiles/manhattan_census_tracts.geojson\")\n",
    "        census_tracts = census_tracts[[\"GEOID\", \"geometry\"]].to_crs(taxi_zones.crs)\n",
    "        print(f\"   ‚úÖ Loaded {len(census_tracts)} Manhattan census tracts\")\n",
    "        \n",
    "        # Create mapping from taxi zones to census tracts using spatial overlays\n",
    "        print(f\"   üîÑ Computing spatial overlaps...\")\n",
    "        \n",
    "        # Method: Use spatial intersection to find overlaps\n",
    "        overlaps = gpd.overlay(taxi_zones, census_tracts, how='intersection')\n",
    "        overlaps['overlap_area'] = overlaps.geometry.area\n",
    "        \n",
    "        # For each taxi zone, find which census tracts it overlaps with\n",
    "        zone_to_tract_mapping = []\n",
    "        \n",
    "        for location_id in manhattan_zones:\n",
    "            zone_overlaps = overlaps[overlaps['LocationID'] == location_id]\n",
    "            \n",
    "            if len(zone_overlaps) > 0:\n",
    "                # Get the tract(s) with the largest overlap area\n",
    "                total_area = zone_overlaps['overlap_area'].sum()\n",
    "                \n",
    "                for _, overlap in zone_overlaps.iterrows():\n",
    "                    # Weight by overlap area\n",
    "                    weight = overlap['overlap_area'] / total_area if total_area > 0 else 1.0\n",
    "                    zone_to_tract_mapping.append({\n",
    "                        'LocationID': location_id,\n",
    "                        'GEOID': overlap['GEOID'],\n",
    "                        'weight': weight\n",
    "                    })\n",
    "            else:\n",
    "                # Fallback: use nearest tract\n",
    "                taxi_zone = taxi_zones[taxi_zones['LocationID'] == location_id]\n",
    "                if len(taxi_zone) > 0:\n",
    "                    zone_centroid = taxi_zone.geometry.centroid.iloc[0]\n",
    "                    distances = census_tracts.geometry.distance(zone_centroid)\n",
    "                    nearest_geoid = census_tracts.iloc[distances.idxmin()]['GEOID']\n",
    "                    \n",
    "                    zone_to_tract_mapping.append({\n",
    "                        'LocationID': location_id,\n",
    "                        'GEOID': nearest_geoid,\n",
    "                        'weight': 1.0\n",
    "                    })\n",
    "        \n",
    "        mapping_df = pd.DataFrame(zone_to_tract_mapping)\n",
    "        print(f\"   ‚úÖ Created {len(mapping_df)} zone‚Üítract mappings\")\n",
    "        \n",
    "        # Apply spatial mapping to create tract-level scores\n",
    "        print(f\"   üîÑ Aggregating taxi scores by census tract...\")\n",
    "        \n",
    "        tract_results = {'GEOID': []}\n",
    "        years = list(files.keys())\n",
    "        \n",
    "        # Get all unique GEOIDs\n",
    "        unique_geoids = mapping_df['GEOID'].unique()\n",
    "        tract_results['GEOID'] = unique_geoids\n",
    "        tract_results['id'] = range(1, len(unique_geoids) + 1)\n",
    "        \n",
    "        # For each time period and year, aggregate scores by tract\n",
    "        periods = ['morning', 'afternoon', 'evening', 'night', 'average']\n",
    "        \n",
    "        for period in periods:\n",
    "            for year in years:\n",
    "                col_name = f'{period}_{year}'\n",
    "                tract_scores = []\n",
    "                \n",
    "                for geoid in unique_geoids:\n",
    "                    # Get all taxi zones that map to this tract\n",
    "                    zone_mappings = mapping_df[mapping_df['GEOID'] == geoid]\n",
    "                    \n",
    "                    # Calculate weighted average of taxi scores\n",
    "                    weighted_score = 0\n",
    "                    total_weight = 0\n",
    "                    \n",
    "                    for _, mapping in zone_mappings.iterrows():\n",
    "                        location_id = mapping['LocationID']\n",
    "                        weight = mapping['weight']\n",
    "                        \n",
    "                        # Find the score for this LocationID\n",
    "                        zone_idx = manhattan_zones.index(location_id)\n",
    "                        zone_score = results[col_name][zone_idx]\n",
    "                        \n",
    "                        weighted_score += zone_score * weight\n",
    "                        total_weight += weight\n",
    "                    \n",
    "                    # Final score for this tract\n",
    "                    final_score = weighted_score / total_weight if total_weight > 0 else 0\n",
    "                    tract_scores.append(final_score)\n",
    "                \n",
    "                tract_results[col_name] = tract_scores\n",
    "        \n",
    "        # Replace results with tract-based data\n",
    "        final_df = pd.DataFrame(tract_results)\n",
    "        print(f\"   ‚úÖ Created tract-based foot traffic scores: {len(final_df)} census tracts\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Spatial files not found: {e}\")\n",
    "        print(f\"   üìù Using simple LocationID‚ÜíGEOID mapping instead...\")\n",
    "        \n",
    "        # Fallback: Simple mapping like your project  \n",
    "        try:\n",
    "            import json\n",
    "            # Use your exact file structure\n",
    "            with open('../census tract geofiles/manhattan_census_tracts.geojson', 'r') as f:\n",
    "                geojson = json.load(f)\n",
    "            geoids = [feature['properties']['GEOID'] for feature in geojson['features']]\n",
    "            \n",
    "            # Simple distribution approach (like your project)\n",
    "            result_rows = []\n",
    "            locations = sorted(manhattan_zones)\n",
    "            tracts_per_location = len(geoids) // len(locations)\n",
    "            \n",
    "            print(f\"   üìä Mapping {len(locations)} LocationIDs to {len(geoids)} GEOIDs\")\n",
    "            print(f\"   üìà Approximately {tracts_per_location} census tracts per LocationID\")\n",
    "            \n",
    "            geoid_index = 0\n",
    "            for i, location_id in enumerate(locations):\n",
    "                num_geoids = tracts_per_location + (1 if i < len(geoids) % len(locations) else 0)\n",
    "                location_idx = manhattan_zones.index(location_id)\n",
    "                \n",
    "                for j in range(num_geoids):\n",
    "                    if geoid_index < len(geoids):\n",
    "                        row = {'GEOID': geoids[geoid_index], 'id': geoid_index + 1}\n",
    "                        \n",
    "                        # Copy all scores from this LocationID\n",
    "                        for col, values in results.items():\n",
    "                            if col not in ['id', 'GEOID']:\n",
    "                                row[col] = values[location_idx]\n",
    "                        \n",
    "                        result_rows.append(row)\n",
    "                        geoid_index += 1\n",
    "            \n",
    "            final_df = pd.DataFrame(result_rows)\n",
    "            print(f\"   ‚úÖ Created simple mapping: {len(final_df)} census tracts\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ‚ùå Fallback mapping failed: {e2}\")\n",
    "            print(f\"   üìù Keeping taxi LocationIDs as GEOID\")\n",
    "            final_df = pd.DataFrame(results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Spatial mapping error: {e}\")\n",
    "        print(f\"   üìù Keeping taxi LocationIDs as GEOID\")\n",
    "        final_df = pd.DataFrame(results)\n",
    "    \n",
    "    # STEP 5: Load subway scores and create combined scores\n",
    "    print(f\"\\nüöá STEP 5: Loading subway scores and creating combined scores...\")\n",
    "    \n",
    "    try:\n",
    "        # Load subway scores by census tract\n",
    "        subway_scores = pd.read_csv('subway_score_by_tract.csv')\n",
    "        print(f\"   ‚úÖ Loaded subway scores: {len(subway_scores)} census tracts\")\n",
    "        \n",
    "        # Create zone-to-tract mapping (simplified - you may need a proper mapping file)\n",
    "        # For now, assume each taxi zone maps to a census tract with similar ID patterns\n",
    "        # You can replace this with actual spatial mapping if needed\n",
    "        final_df['GEOID'] = final_df['GEOID'].astype(str)\n",
    "        subway_scores['GEOID'] = subway_scores['GEOID'].astype(str)\n",
    "        \n",
    "        # Merge subway scores\n",
    "        final_df = final_df.merge(subway_scores, on='GEOID', how='left')\n",
    "        final_df['subway_score'] = final_df['subway_score'].fillna(0)\n",
    "        \n",
    "        # Create combined scores for each time period and year\n",
    "        score_columns = [col for col in final_df.columns \n",
    "                        if col.endswith(tuple(str(y) for y in years)) \n",
    "                        and col != 'subway_score']\n",
    "        \n",
    "        for col in score_columns:\n",
    "            combined_col = col.replace('_', '_combined_')\n",
    "            final_df[combined_col] = (\n",
    "                0.65 * final_df[col] + \n",
    "                0.35 * final_df['subway_score']\n",
    "            ).round(3)\n",
    "        \n",
    "        print(f\"   üîÑ Created {len(score_columns)} combined scores: taxi (65%) + subway (35%)\")\n",
    "        print(f\"   üìä Combined score range: {final_df[score_columns[0].replace('_', '_combined_')].min():.2f} - {final_df[score_columns[0].replace('_', '_combined_')].max():.2f}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ‚ö†Ô∏è  subway_score_by_tract.csv not found - using taxi scores only\")\n",
    "        print(f\"   üìù Create subway scores first: run MTA subway processing\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error loading subway scores: {e}\")\n",
    "        print(f\"   üìù Continuing with taxi-only scores\")\n",
    "    \n",
    "    # STEP 6: Save and analyze results\n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_df.to_csv('foot_traffic_ml_optimized.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ SAVED: foot_traffic_ml_optimized.csv\")\n",
    "    print(f\"üìä {len(final_df)} zones √ó {len(final_df.columns)} columns\")\n",
    "    \n",
    "    # STEP 7: Analyze for ML readiness\n",
    "    print(f\"\\nü§ñ ML READINESS ANALYSIS:\")\n",
    "    \n",
    "    # Analyze taxi-only scores\n",
    "    taxi_score_cols = [col for col in final_df.columns \n",
    "                      if col.endswith(tuple(str(y) for y in years)) \n",
    "                      and 'combined' not in col \n",
    "                      and col != 'subway_score']\n",
    "    \n",
    "    if len(taxi_score_cols) > 0:\n",
    "        taxi_scores = final_df[taxi_score_cols].values.flatten()\n",
    "        print(f\"   üöï Taxi scores: {taxi_scores.min():.1f} - {taxi_scores.max():.1f} (mean: {taxi_scores.mean():.1f})\")\n",
    "    \n",
    "    # Analyze combined scores if available\n",
    "    combined_score_cols = [col for col in final_df.columns if 'combined' in col]\n",
    "    if len(combined_score_cols) > 0:\n",
    "        combined_scores = final_df[combined_score_cols].values.flatten()\n",
    "        print(f\"   üöá Combined scores: {combined_scores.min():.1f} - {combined_scores.max():.1f} (mean: {combined_scores.mean():.1f})\")\n",
    "        \n",
    "        # Show the effect of subway combination\n",
    "        if len(taxi_score_cols) > 0:\n",
    "            print(f\"   üìà Subway effect: Mean combined vs taxi = {combined_scores.mean():.1f} vs {taxi_scores.mean():.1f}\")\n",
    "    \n",
    "    # Check temporal patterns (this is what was broken before!)\n",
    "    print(f\"\\n‚è∞ TEMPORAL PATTERN EXAMPLE (Zone 161 - highest activity):\")\n",
    "    zone_161 = final_df[final_df['GEOID'] == 161].iloc[0]\n",
    "    \n",
    "    for period in ['morning', 'afternoon', 'evening']:\n",
    "        # Show taxi scores\n",
    "        taxi_cols = [f'{period}_{year}' for year in years if f'{period}_{year}' in final_df.columns]\n",
    "        if taxi_cols:\n",
    "            taxi_scores = [zone_161[col] for col in taxi_cols]\n",
    "            frontend_scores = [s * 10 for s in taxi_scores]  # What frontend will show\n",
    "            trend = \"‚ÜóÔ∏è\" if taxi_scores[-1] > taxi_scores[0] else \"‚ÜòÔ∏è\" if taxi_scores[-1] < taxi_scores[0] else \"‚û°Ô∏è\"\n",
    "            print(f\"   üöï {period:10}: {' ‚Üí '.join([f'{s:.1f}' for s in taxi_scores])} (frontend: {' ‚Üí '.join([f'{s:.0f}' for s in frontend_scores])}) {trend}\")\n",
    "        \n",
    "        # Show combined scores if available\n",
    "        combined_cols = [f'{period}_combined_{year}' for year in years if f'{period}_combined_{year}' in final_df.columns]\n",
    "        if combined_cols:\n",
    "            combined_scores = [zone_161[col] for col in combined_cols]\n",
    "            frontend_combined = [s * 10 for s in combined_scores]\n",
    "            trend = \"‚ÜóÔ∏è\" if combined_scores[-1] > combined_scores[0] else \"‚ÜòÔ∏è\" if combined_scores[-1] < combined_scores[0] else \"‚û°Ô∏è\"\n",
    "            print(f\"   üöá {period:10}: {' ‚Üí '.join([f'{s:.1f}' for s in combined_scores])} (frontend: {' ‚Üí '.join([f'{s:.0f}' for s in frontend_combined])}) {trend}\")\n",
    "    \n",
    "    print(f\"   ‚Üë These trends are now MEANINGFUL for ML!\")\n",
    "    \n",
    "    # Show zones suitable for different activity levels\n",
    "    print(f\"\\nüéØ ZONE ACTIVITY DISTRIBUTION:\")\n",
    "    avg_cols = [f'average_{year}' for year in years]\n",
    "    final_df['overall_avg'] = final_df[avg_cols].mean(axis=1)\n",
    "    \n",
    "    activity_levels = {\n",
    "        'Very High (8-10)': (final_df['overall_avg'] >= 8).sum(),\n",
    "        'High (6-8)': ((final_df['overall_avg'] >= 6) & (final_df['overall_avg'] < 8)).sum(),\n",
    "        'Medium (4-6)': ((final_df['overall_avg'] >= 4) & (final_df['overall_avg'] < 6)).sum(),\n",
    "        'Low (2-4)': ((final_df['overall_avg'] >= 2) & (final_df['overall_avg'] < 4)).sum(),\n",
    "        'Very Low (0-2)': (final_df['overall_avg'] < 2).sum()\n",
    "    }\n",
    "    \n",
    "    for level, count in activity_levels.items():\n",
    "        print(f\"   {level}: {count} zones\")\n",
    "    \n",
    "    # ML-specific recommendations\n",
    "    print(f\"\\n‚úÖ ML OPTIMIZATION RESULTS:\")\n",
    "    print(f\"   üéØ Global normalization preserves temporal relationships\")\n",
    "    print(f\"   üìà Year-over-year trends are now meaningful\") \n",
    "    print(f\"   ‚è∞ Time period comparisons work across years\")\n",
    "    print(f\"   üî¢ 0-10 scale (frontend √ó10 = your chart values)\")\n",
    "    print(f\"   üöá Combined taxi (65%) + subway (35%) scores when available\")\n",
    "    print(f\"   üìÖ 4-year dataset (2020-2023) for robust trend learning\")\n",
    "    print(f\"   ü§ñ Ready for time series ML models\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def create_trend_features_for_ml(df):\n",
    "    \"\"\"\n",
    "    Add ML-specific trend features to the optimized dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîß ADDING ML TREND FEATURES...\")\n",
    "    \n",
    "    years = [2020, 2021, 2022, 2023]\n",
    "    periods = ['morning', 'afternoon', 'evening', 'night', 'average']\n",
    "    \n",
    "    # Add trend features\n",
    "    for period in periods:\n",
    "        period_cols = [f'{period}_{year}' for year in years if f'{period}_{year}' in df.columns]\n",
    "        \n",
    "        if len(period_cols) >= 2:\n",
    "            # Linear trend (slope)\n",
    "            trends = []\n",
    "            for idx, row in df.iterrows():\n",
    "                values = [row[col] for col in period_cols]\n",
    "                x = np.array(range(len(values)))\n",
    "                trend = np.polyfit(x, values, 1)[0] if len(values) >= 2 else 0\n",
    "                trends.append(trend)\n",
    "            \n",
    "            df[f'{period}_trend_slope'] = trends\n",
    "            \n",
    "            # Year-over-year growth rate\n",
    "            if len(period_cols) >= 2:\n",
    "                df[f'{period}_growth_rate'] = ((df[period_cols[-1]] - df[period_cols[0]]) / (df[period_cols[0]] + 1)) * 100\n",
    "    \n",
    "    # Add seasonal patterns\n",
    "    df['prefers_morning'] = df[[f'morning_{y}' for y in years]].mean(axis=1)\n",
    "    df['prefers_afternoon'] = df[[f'afternoon_{y}' for y in years]].mean(axis=1)\n",
    "    df['prefers_evening'] = df[[f'evening_{y}' for y in years]].mean(axis=1)\n",
    "    df['prefers_night'] = df[[f'night_{y}' for y in years]].mean(axis=1)\n",
    "    \n",
    "    # Find peak period for each zone\n",
    "    time_cols = ['prefers_morning', 'prefers_afternoon', 'prefers_evening', 'prefers_night']\n",
    "    df['peak_period'] = df[time_cols].idxmax(axis=1).str.replace('prefers_', '')\n",
    "    \n",
    "    # Save enhanced version\n",
    "    df.to_csv('foot_traffic_ml_ready.csv', index=False)\n",
    "    \n",
    "    print(f\"   ‚úÖ Added trend slopes, growth rates, seasonal preferences (4-year data)\")\n",
    "    print(f\"   üíæ Saved: foot_traffic_ml_ready.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ RUNNING ML-OPTIMIZED PROCESSOR\")\n",
    "    print(\"Goal: 0-10 decimal scores + taxi/subway combination + ML-ready temporal patterns\")\n",
    "    print(\"Data: 2020-2023 (4 years) | Combination: 65% taxi + 35% subway\")\n",
    "    print(\"üìÇ Run from: foot_traffic_score/ directory\")\n",
    "    print()\n",
    "    \n",
    "    # Process with global normalization\n",
    "    optimized_df = ml_optimized_foot_traffic_processor()\n",
    "    \n",
    "    # Add ML features\n",
    "    ml_ready_df = create_trend_features_for_ml(optimized_df)\n",
    "    \n",
    "    print(f\"\\nüéâ COMPLETE!\")\n",
    "    print(f\"‚úÖ foot_traffic_ml_optimized.csv - All scores (taxi + combined)\")\n",
    "    print(f\"‚úÖ foot_traffic_taxi_only.csv - Taxi scores only\") \n",
    "    print(f\"‚úÖ foot_traffic_combined_only.csv - Combined scores only (if subway available)\")\n",
    "    print(f\"‚úÖ foot_traffic_ml_ready.csv - Enhanced with ML features\") \n",
    "    print(f\"üìÇ All files saved to: foot_traffic_score/ directory\")\n",
    "    print(f\"üìä Frontend: multiply by 10 for display (9.3 ‚Üí 93)\")\n",
    "    print(f\"üöá Combination: 65% taxi + 35% subway (following your project pattern)\")\n",
    "    print(f\"ü§ñ Optimized for time series ML models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FIXED Foot Traffic ML Pipeline\n",
      "==================================================\n",
      "\n",
      "üìÇ Step 1: Loading Data...\n",
      "‚úÖ Loaded: foot_traffic_ml_ready.csv\n",
      "üìä Original dataset shape: (49, 38)\n",
      "üìä Columns: ['id', 'GEOID', 'morning_2020', 'afternoon_2020', 'evening_2020', 'night_2020', 'average_2020', 'morning_2021', 'afternoon_2021', 'evening_2021', 'night_2021', 'average_2021', 'morning_2022', 'afternoon_2022', 'evening_2022', 'night_2022', 'average_2022', 'morning_2023', 'afternoon_2023', 'evening_2023', 'night_2023', 'average_2023', 'overall_avg', 'morning_trend_slope', 'morning_growth_rate', 'afternoon_trend_slope', 'afternoon_growth_rate', 'evening_trend_slope', 'evening_growth_rate', 'night_trend_slope', 'night_growth_rate', 'average_trend_slope', 'average_growth_rate', 'prefers_morning', 'prefers_afternoon', 'prefers_evening', 'prefers_night', 'peak_period']\n",
      "üìä Sample data:\n",
      "   id  GEOID  morning_2020  afternoon_2020  evening_2020  night_2020  \\\n",
      "0   1      4      0.099753        0.227694      0.276751    0.101110   \n",
      "1   2     12      0.027886        0.023444      0.008507    0.003342   \n",
      "2   3     13      0.382973        0.430579      0.346035    0.055737   \n",
      "3   4     14      0.009537        0.023296      0.035311    0.013701   \n",
      "4   5     24      0.133564        0.224228      0.180157    0.038505   \n",
      "\n",
      "   average_2020  morning_2021  afternoon_2021  evening_2021  ...  \\\n",
      "0      0.707390      0.108697        0.288499      0.330167  ...   \n",
      "1      0.065262      0.042847        0.042207      0.011456  ...   \n",
      "2      1.217405      0.390345        0.563092      0.411087  ...   \n",
      "3      0.083927      0.009699        0.025369      0.031224  ...   \n",
      "4      0.578536      0.147271        0.284326      0.227077  ...   \n",
      "\n",
      "   evening_growth_rate  night_trend_slope  night_growth_rate  \\\n",
      "0             9.283378           0.031144           7.423110   \n",
      "1             0.346001          -0.000497          -0.176327   \n",
      "2            16.357387           0.018373           4.632167   \n",
      "3             0.497179           0.001653           0.403037   \n",
      "4             5.076007           0.011201           2.555532   \n",
      "\n",
      "   average_trend_slope  average_growth_rate  prefers_morning  \\\n",
      "0             0.074599            11.672022         0.101945   \n",
      "1             0.036529             9.258683         0.060857   \n",
      "2             0.283753            33.012819         0.507369   \n",
      "3             0.002320             0.352174         0.008350   \n",
      "4             0.031105             4.424485         0.140608   \n",
      "\n",
      "   prefers_afternoon  prefers_evening  prefers_night  peak_period  \n",
      "0           0.256269         0.357428       0.136271      evening  \n",
      "1           0.048265         0.011418       0.002356      morning  \n",
      "2           0.604435         0.472141       0.080722    afternoon  \n",
      "3           0.022959         0.037713       0.016936      evening  \n",
      "4           0.250834         0.234979       0.058719    afternoon  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "\n",
      "üîÑ Step 2: Reshaping Data for ML...\n",
      "üìä Found time period columns: 16\n",
      "üìä Examples: ['morning_2020', 'afternoon_2020', 'evening_2020', 'night_2020', 'morning_2021']\n",
      "üìä Created 784 ML training rows\n",
      "üìä ML DataFrame shape: (784, 19)\n",
      "üìä Unique GEOIDs: 49\n",
      "üìä Years: [np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023)]\n",
      "üìä Time periods: ['afternoon', 'evening', 'morning', 'night']\n",
      "üìä Score range: 0.000 to 4.008\n",
      "\n",
      "üîß Step 3: Feature Engineering...\n",
      "‚úÖ Added 15 trend features\n",
      "üìã Final features (18): ['geoid_encoded', 'period_encoded', 'year_normalized', 'morning_trend_slope', 'morning_growth_rate', 'afternoon_trend_slope', 'afternoon_growth_rate', 'evening_trend_slope', 'evening_growth_rate', 'night_trend_slope', 'night_growth_rate', 'average_trend_slope', 'average_growth_rate', 'prefers_morning', 'prefers_afternoon', 'prefers_evening', 'prefers_night', 'overall_avg']\n",
      "üìä Final training data: X=(784, 18), y=(784,)\n",
      "\n",
      "ü§ñ Step 4: Testing Models...\n",
      "üìä Training set: (627, 18)\n",
      "üìä Test set: (157, 18)\n",
      "\n",
      "--- Training RandomForest ---\n",
      "RandomForest ‚Üí R¬≤: 0.9585, MAE: 0.0810, RMSE: 0.1307\n",
      "\n",
      "--- Training HistGradientBoosting ---\n",
      "HistGradientBoosting ‚Üí R¬≤: 0.9534, MAE: 0.0700, RMSE: 0.1385\n",
      "\n",
      "--- Training GradientBoosting ---\n",
      "GradientBoosting ‚Üí R¬≤: 0.9287, MAE: 0.1107, RMSE: 0.1713\n",
      "\n",
      "--- Training LinearRegression ---\n",
      "LinearRegression ‚Üí R¬≤: 0.7231, MAE: 0.2608, RMSE: 0.3375\n",
      "\n",
      "--- Training Ridge ---\n",
      "Ridge ‚Üí R¬≤: 0.7299, MAE: 0.2582, RMSE: 0.3333\n",
      "\n",
      "--- Training DecisionTree ---\n",
      "DecisionTree ‚Üí R¬≤: 0.9312, MAE: 0.0921, RMSE: 0.1682\n",
      "\n",
      "--- Training KNN ---\n",
      "KNN ‚Üí R¬≤: 0.8593, MAE: 0.1561, RMSE: 0.2406\n",
      "\n",
      "üèÜ Step 5: Results Summary\n",
      "============================================================\n",
      "RandomForest         | R¬≤: 0.9585 | MAE: 0.0810 | RMSE: 0.1307\n",
      "HistGradientBoosting | R¬≤: 0.9534 | MAE: 0.0700 | RMSE: 0.1385\n",
      "DecisionTree         | R¬≤: 0.9312 | MAE: 0.0921 | RMSE: 0.1682\n",
      "GradientBoosting     | R¬≤: 0.9287 | MAE: 0.1107 | RMSE: 0.1713\n",
      "KNN                  | R¬≤: 0.8593 | MAE: 0.1561 | RMSE: 0.2406\n",
      "Ridge                | R¬≤: 0.7299 | MAE: 0.2582 | RMSE: 0.3333\n",
      "LinearRegression     | R¬≤: 0.7231 | MAE: 0.2608 | RMSE: 0.3375\n",
      "\n",
      "ü•á Best Model: RandomForest (R¬≤ = 0.9585)\n",
      "\n",
      "üîÆ Step 6: Generating Future Predictions...\n",
      "üìç Predicting for 49 GEOIDs, 4 time periods, 3 years\n",
      "\n",
      "üìä Step 7: Formatting Output...\n",
      "üìä Final output shape: (49, 16)\n",
      "üìä Prediction columns: ['afternoon_pred_2025', 'afternoon_pred_2026', 'afternoon_pred_2027', 'evening_pred_2025', 'evening_pred_2026', 'evening_pred_2027', 'morning_pred_2025', 'morning_pred_2026', 'morning_pred_2027', 'night_pred_2025', 'night_pred_2026', 'night_pred_2027', 'average_pred_2025', 'average_pred_2026', 'average_pred_2027']\n",
      "\n",
      "üìä Step 8: Creating Final Database Format...\n",
      "   Adding predictions for 2024...\n",
      "   Adding predictions for 2025...\n",
      "   Adding predictions for 2026...\n",
      "   Adding predictions for 2027...\n",
      "üîß Applied scale factor: 2.605\n",
      "‚úÖ Saved: tract_foot_traffic_trends_rows 1.csv\n",
      "üìä Final clean format: (49, 34)\n",
      "üìä Columns: 34 (target: 34)\n",
      "\n",
      "üìã Sample of clean output:\n",
      "   id  GEOID  morning_2024  morning_pred_2025  afternoon_2024  \\\n",
      "0   1      4      0.292504           0.292504        0.687152   \n",
      "1   2     12      0.183956           0.183956        0.146300   \n",
      "2   3     13      1.506519           1.506519        1.732783   \n",
      "3   4     14      0.025993           0.025993        0.058487   \n",
      "4   5     24      0.376523           0.376523        0.714283   \n",
      "\n",
      "   afternoon_pred_2025  evening_2024  evening_pred_2025  \n",
      "0             0.687152      0.917815           0.917815  \n",
      "1             0.146300      0.114348           0.114348  \n",
      "2             1.732783      1.460021           1.460021  \n",
      "3             0.058487      0.100055           0.100055  \n",
      "4             0.714283      0.746441           0.746441  \n",
      "\n",
      "üéâ FIXED Pipeline Complete!\n",
      "üèÜ Best model: RandomForest (R¬≤ = 0.9585)\n",
      "üìä Output ready for database!\n",
      "\n",
      "üìã Sample of clean output:\n",
      "   id  GEOID  morning_2024  morning_pred_2025  afternoon_2024  \\\n",
      "0   1      4      0.292504           0.292504        0.687152   \n",
      "1   2     12      0.183956           0.183956        0.146300   \n",
      "2   3     13      1.506519           1.506519        1.732783   \n",
      "3   4     14      0.025993           0.025993        0.058487   \n",
      "4   5     24      0.376523           0.376523        0.714283   \n",
      "\n",
      "   afternoon_pred_2025  evening_2024  evening_pred_2025  \n",
      "0             0.687152      0.917815           0.917815  \n",
      "1             0.146300      0.114348           0.114348  \n",
      "2             1.732783      1.460021           1.460021  \n",
      "3             0.058487      0.100055           0.100055  \n",
      "4             0.714283      0.746441           0.746441  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def fixed_foot_traffic_ml_pipeline():\n",
    "    \"\"\"\n",
    "    FIXED ML pipeline that works with your actual data structure\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ FIXED Foot Traffic ML Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Load data with better error handling\n",
    "    print(\"\\nüìÇ Step 1: Loading Data...\")\n",
    "    \n",
    "    df = None\n",
    "    use_ml_features = False\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv('foot_traffic_ml_ready.csv')\n",
    "        print(f\"‚úÖ Loaded: foot_traffic_ml_ready.csv\")\n",
    "        use_ml_features = True\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            df = pd.read_csv('foot_traffic_ml_optimized.csv')\n",
    "            print(f\"‚úÖ Loaded: foot_traffic_ml_optimized.csv\")\n",
    "            use_ml_features = False\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ùå Could not find data files!\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"üìä Original dataset shape: {df.shape}\")\n",
    "    print(f\"üìä Columns: {df.columns.tolist()}\")\n",
    "    print(f\"üìä Sample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Step 2: Reshape data properly\n",
    "    print(f\"\\nüîÑ Step 2: Reshaping Data for ML...\")\n",
    "    \n",
    "    # Identify time period columns automatically\n",
    "    time_columns = []\n",
    "    for col in df.columns:\n",
    "        if any(period in col for period in ['morning', 'afternoon', 'evening', 'night']) and any(str(year) in col for year in [2020, 2021, 2022, 2023]):\n",
    "            time_columns.append(col)\n",
    "    \n",
    "    print(f\"üìä Found time period columns: {len(time_columns)}\")\n",
    "    print(f\"üìä Examples: {time_columns[:5]}\")\n",
    "    \n",
    "    # Reshape to long format\n",
    "    reshaped_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        geoid = row['GEOID']\n",
    "        \n",
    "        for col in time_columns:\n",
    "            if pd.notna(row[col]):  # Only process non-NaN values\n",
    "                # Parse column name to extract period and year\n",
    "                parts = col.split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    period = parts[0]\n",
    "                    year = int(parts[1])\n",
    "                    score = row[col]\n",
    "                    \n",
    "                    # Create ML row\n",
    "                    ml_row = {\n",
    "                        'GEOID': geoid,\n",
    "                        'year': year,\n",
    "                        'time_period': period,\n",
    "                        'foot_traffic_score': score\n",
    "                    }\n",
    "                    \n",
    "                    # Add trend features if available\n",
    "                    if use_ml_features:\n",
    "                        trend_cols = [c for c in df.columns if 'trend' in c or 'growth' in c or 'prefers' in c]\n",
    "                        for trend_col in trend_cols:\n",
    "                            if trend_col in df.columns:\n",
    "                                ml_row[trend_col] = row[trend_col]\n",
    "                        \n",
    "                        # Add overall average if available\n",
    "                        if 'overall_avg' in df.columns:\n",
    "                            ml_row['overall_avg'] = row['overall_avg']\n",
    "                    \n",
    "                    reshaped_data.append(ml_row)\n",
    "    \n",
    "    print(f\"üìä Created {len(reshaped_data)} ML training rows\")\n",
    "    \n",
    "    if len(reshaped_data) == 0:\n",
    "        print(\"‚ùå No valid training data created!\")\n",
    "        print(\"üîç Debug info:\")\n",
    "        print(f\"   Time columns found: {time_columns}\")\n",
    "        print(f\"   Sample values: {[df[col].iloc[0] if col in df.columns else 'N/A' for col in time_columns[:3]]}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    ml_df = pd.DataFrame(reshaped_data)\n",
    "    \n",
    "    print(f\"üìä ML DataFrame shape: {ml_df.shape}\")\n",
    "    print(f\"üìä Unique GEOIDs: {ml_df['GEOID'].nunique()}\")\n",
    "    print(f\"üìä Years: {sorted(ml_df['year'].unique())}\")\n",
    "    print(f\"üìä Time periods: {sorted(ml_df['time_period'].unique())}\")\n",
    "    print(f\"üìä Score range: {ml_df['foot_traffic_score'].min():.3f} to {ml_df['foot_traffic_score'].max():.3f}\")\n",
    "    \n",
    "    # Step 3: Feature engineering\n",
    "    print(f\"\\nüîß Step 3: Feature Engineering...\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_geoid = LabelEncoder()\n",
    "    le_period = LabelEncoder()\n",
    "    \n",
    "    ml_df['geoid_encoded'] = le_geoid.fit_transform(ml_df['GEOID'])\n",
    "    ml_df['period_encoded'] = le_period.fit_transform(ml_df['time_period'])\n",
    "    \n",
    "    # Create time-based features\n",
    "    ml_df['year_normalized'] = (ml_df['year'] - ml_df['year'].min()) / (ml_df['year'].max() - ml_df['year'].min())\n",
    "    \n",
    "    # Basic features that always exist\n",
    "    base_features = ['geoid_encoded', 'period_encoded', 'year_normalized']\n",
    "    \n",
    "    # Add trend features if available\n",
    "    feature_columns = base_features.copy()\n",
    "    if use_ml_features:\n",
    "        trend_features = [col for col in ml_df.columns if col not in ['GEOID', 'year', 'time_period', 'foot_traffic_score'] + base_features]\n",
    "        feature_columns.extend(trend_features)\n",
    "        print(f\"‚úÖ Added {len(trend_features)} trend features\")\n",
    "    \n",
    "    target_column = 'foot_traffic_score'\n",
    "    \n",
    "    print(f\"üìã Final features ({len(feature_columns)}): {feature_columns}\")\n",
    "    \n",
    "    # Prepare final data\n",
    "    X = ml_df[feature_columns].copy()\n",
    "    y = ml_df[target_column].copy()\n",
    "    \n",
    "    # Handle any remaining NaN values\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    print(f\"üìä Final training data: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    # Step 4: Model testing\n",
    "    print(f\"\\nü§ñ Step 4: Testing Models...\")\n",
    "    \n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        \"HistGradientBoosting\": HistGradientBoostingRegressor(random_state=42),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(random_state=42, n_estimators=100),\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(alpha=1.0),\n",
    "        \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "        \"KNN\": KNeighborsRegressor(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"üìä Training set: {X_train.shape}\")\n",
    "    print(f\"üìä Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Test models\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_model_obj = None\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            print(f\"\\n--- Training {name} ---\")\n",
    "            \n",
    "            # Fit model\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Metrics\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'R¬≤': r2,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse\n",
    "            })\n",
    "            \n",
    "            print(f\"{name} ‚Üí R¬≤: {r2:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "            \n",
    "            # Track best model\n",
    "            if r2 > best_score:\n",
    "                best_score = r2\n",
    "                best_model = name\n",
    "                best_model_obj = model\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {name} failed: {e}\")\n",
    "    \n",
    "    # Step 5: Results\n",
    "    print(f\"\\nüèÜ Step 5: Results Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('R¬≤', ascending=False)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"{row['Model'].ljust(20)} | R¬≤: {row['R¬≤']:.4f} | MAE: {row['MAE']:.4f} | RMSE: {row['RMSE']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nü•á Best Model: {best_model} (R¬≤ = {best_score:.4f})\")\n",
    "    \n",
    "    # Step 6: Generate predictions\n",
    "    print(f\"\\nüîÆ Step 6: Generating Future Predictions...\")\n",
    "    \n",
    "    future_years = [2025, 2026, 2027]\n",
    "    unique_geoids = df['GEOID'].unique()\n",
    "    unique_periods = ml_df['time_period'].unique()\n",
    "    \n",
    "    print(f\"üìç Predicting for {len(unique_geoids)} GEOIDs, {len(unique_periods)} time periods, {len(future_years)} years\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    future_predictions = []\n",
    "    \n",
    "    for year in future_years:\n",
    "        for geoid in unique_geoids:\n",
    "            for period in unique_periods:\n",
    "                # Create prediction row\n",
    "                pred_row = {\n",
    "                    'geoid_encoded': le_geoid.transform([geoid])[0],\n",
    "                    'period_encoded': le_period.transform([period])[0],\n",
    "                    'year_normalized': (year - ml_df['year'].min()) / (ml_df['year'].max() - ml_df['year'].min())\n",
    "                }\n",
    "                \n",
    "                # Add trend features if available\n",
    "                if use_ml_features:\n",
    "                    geoid_data = df[df['GEOID'] == geoid].iloc[0]\n",
    "                    for feat in trend_features:\n",
    "                        if feat in geoid_data:\n",
    "                            pred_row[feat] = geoid_data[feat]\n",
    "                        else:\n",
    "                            pred_row[feat] = 0\n",
    "                \n",
    "                # Make prediction\n",
    "                pred_features = pd.DataFrame([pred_row])[feature_columns]\n",
    "                pred_features = pred_features.fillna(pred_features.median())\n",
    "                pred_scaled = scaler.transform(pred_features)\n",
    "                \n",
    "                prediction = best_model_obj.predict(pred_scaled)[0]\n",
    "                \n",
    "                future_predictions.append({\n",
    "                    'GEOID': geoid,\n",
    "                    'year': year,\n",
    "                    'time_period': period,\n",
    "                    'predicted_score': prediction\n",
    "                })\n",
    "    \n",
    "    # Step 7: Format output\n",
    "    print(f\"\\nüìä Step 7: Formatting Output...\")\n",
    "    \n",
    "    pred_df = pd.DataFrame(future_predictions)\n",
    "    \n",
    "    # Pivot to database format\n",
    "    pivot_df = pred_df.pivot_table(\n",
    "        index='GEOID',\n",
    "        columns=['time_period', 'year'],\n",
    "        values='predicted_score',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Flatten column names\n",
    "    pivot_df.columns = [f'{period}_pred_{year}' for period, year in pivot_df.columns]\n",
    "    pivot_df = pivot_df.reset_index()\n",
    "    \n",
    "    # Add average predictions\n",
    "    for year in future_years:\n",
    "        year_cols = [col for col in pivot_df.columns if f'pred_{year}' in col and 'average' not in col]\n",
    "        if year_cols:\n",
    "            pivot_df[f'average_pred_{year}'] = pivot_df[year_cols].mean(axis=1)\n",
    "    \n",
    "    # Ensure 0-10 range\n",
    "    pred_cols = [col for col in pivot_df.columns if 'pred_' in col]\n",
    "    for col in pred_cols:\n",
    "        pivot_df[col] = np.clip(pivot_df[col], 0, 10)\n",
    "    \n",
    "    print(f\"üìä Final output shape: {pivot_df.shape}\")\n",
    "    print(f\"üìä Prediction columns: {[col for col in pivot_df.columns if 'pred_' in col]}\")\n",
    "    \n",
    "    # Step 8: Create final format matching target structure\n",
    "    print(f\"\\nüìä Step 8: Creating Final Database Format...\")\n",
    "    \n",
    "    # Start fresh with clean format\n",
    "    unique_geoids = df['GEOID'].unique()\n",
    "    clean_df = pd.DataFrame({'GEOID': unique_geoids})\n",
    "    clean_df['id'] = range(1, len(clean_df) + 1)\n",
    "    \n",
    "    # Add historical data (2020-2023) from original data\n",
    "    time_periods_clean = ['morning', 'afternoon', 'evening']  # Match target format\n",
    "    historical_years = [2020, 2021, 2022, 2023]  # Only years you actually have\n",
    "    \n",
    "    for period in time_periods_clean:\n",
    "        for year in historical_years:\n",
    "            col_name = f'{period}_{year}'\n",
    "            if col_name in df.columns:\n",
    "                clean_df = clean_df.merge(\n",
    "                    df[['GEOID', col_name]], \n",
    "                    on='GEOID', \n",
    "                    how='left'\n",
    "                )\n",
    "            else:\n",
    "                # Fill missing years with zeros or interpolated values\n",
    "                clean_df[col_name] = 0.0\n",
    "    \n",
    "    # Add average columns for historical years\n",
    "    for year in historical_years:\n",
    "        year_cols = [f'{period}_{year}' for period in time_periods_clean if f'{period}_{year}' in clean_df.columns]\n",
    "        if year_cols:\n",
    "            clean_df[f'average_{year}'] = clean_df[year_cols].mean(axis=1)\n",
    "        else:\n",
    "            clean_df[f'average_{year}'] = 0.0\n",
    "    \n",
    "    # Add predictions from ML model (2024-2027)\n",
    "    prediction_years = [2024, 2025, 2026, 2027] \n",
    "    \n",
    "    # Generate predictions for each year\n",
    "    for year in prediction_years:\n",
    "        print(f\"   Adding predictions for {year}...\")\n",
    "        \n",
    "        for period in time_periods_clean:\n",
    "            period_predictions = []\n",
    "            \n",
    "            for geoid in unique_geoids:\n",
    "                # Create prediction features\n",
    "                pred_row = {\n",
    "                    'geoid_encoded': le_geoid.transform([geoid])[0],\n",
    "                    'period_encoded': le_period.transform([period])[0],\n",
    "                    'year_normalized': (year - ml_df['year'].min()) / (ml_df['year'].max() - ml_df['year'].min())\n",
    "                }\n",
    "                \n",
    "                # Add trend features if available\n",
    "                if use_ml_features:\n",
    "                    geoid_data = df[df['GEOID'] == geoid].iloc[0]\n",
    "                    for feat in trend_features:\n",
    "                        if feat in geoid_data:\n",
    "                            pred_row[feat] = geoid_data[feat]\n",
    "                        else:\n",
    "                            pred_row[feat] = 0\n",
    "                \n",
    "                # Make prediction\n",
    "                pred_features = pd.DataFrame([pred_row])[feature_columns]\n",
    "                pred_features = pred_features.fillna(pred_features.median())\n",
    "                pred_scaled = scaler.transform(pred_features)\n",
    "                \n",
    "                prediction = best_model_obj.predict(pred_scaled)[0]\n",
    "                period_predictions.append(prediction)\n",
    "            \n",
    "            # Add to clean dataframe\n",
    "            if year == 2024:\n",
    "                col_name = f'{period}_{year}'\n",
    "            else:\n",
    "                col_name = f'{period}_pred_{year}'\n",
    "            \n",
    "            clean_df[col_name] = period_predictions\n",
    "    \n",
    "    # Add average predictions\n",
    "    for year in prediction_years:\n",
    "        if year == 2024:\n",
    "            year_cols = [f'{period}_{year}' for period in time_periods_clean]\n",
    "            clean_df[f'average_{year}'] = clean_df[year_cols].mean(axis=1)\n",
    "        else:\n",
    "            year_cols = [f'{period}_pred_{year}' for period in time_periods_clean]\n",
    "            clean_df[f'average_pred_{year}'] = clean_df[year_cols].mean(axis=1)\n",
    "    \n",
    "    # Scale predictions to 0-10 range\n",
    "    pred_cols = []\n",
    "    for year in prediction_years:\n",
    "        for period in time_periods_clean:\n",
    "            if year == 2024:\n",
    "                pred_cols.append(f'{period}_{year}')\n",
    "            else:\n",
    "                pred_cols.append(f'{period}_pred_{year}')\n",
    "        \n",
    "        if year == 2024:\n",
    "            pred_cols.append(f'average_{year}')\n",
    "        else:\n",
    "            pred_cols.append(f'average_pred_{year}')\n",
    "    \n",
    "    # Apply scaling to predictions only\n",
    "    if pred_cols:\n",
    "        pred_data = clean_df[pred_cols]\n",
    "        current_max = pred_data.max().max()\n",
    "        if current_max > 0:\n",
    "            scale_factor = 10.0 / current_max\n",
    "            clean_df[pred_cols] = clean_df[pred_cols] * scale_factor\n",
    "            print(f\"üîß Applied scale factor: {scale_factor:.3f}\")\n",
    "    \n",
    "    # Ensure exact column order matching target format\n",
    "    expected_columns = ['id', 'GEOID']\n",
    "    \n",
    "    # Add time period columns in order (2020-2023 + 2024-2027)\n",
    "    all_years = [2020, 2021, 2022, 2023, 2024, 'pred_2025', 'pred_2026', 'pred_2027']\n",
    "    \n",
    "    for period in time_periods_clean:\n",
    "        for year in all_years:\n",
    "            if year == 2024:\n",
    "                col_name = f'{period}_{year}'\n",
    "            elif isinstance(year, str):  # pred_YYYY\n",
    "                col_name = f'{period}_{year}'\n",
    "            else:\n",
    "                col_name = f'{period}_{year}'\n",
    "            \n",
    "            if col_name in clean_df.columns:\n",
    "                expected_columns.append(col_name)\n",
    "    \n",
    "    # Add average columns\n",
    "    for year in all_years:\n",
    "        if year == 2024:\n",
    "            col_name = f'average_{year}'\n",
    "        elif isinstance(year, str):  # pred_YYYY\n",
    "            col_name = f'average_{year}'\n",
    "        else:\n",
    "            col_name = f'average_{year}'\n",
    "        \n",
    "        if col_name in clean_df.columns:\n",
    "            expected_columns.append(col_name)\n",
    "    \n",
    "    # Reorder and clean up\n",
    "    for col in expected_columns:\n",
    "        if col not in clean_df.columns:\n",
    "            clean_df[col] = 0.0\n",
    "    \n",
    "    final_clean_df = clean_df[expected_columns]\n",
    "    \n",
    "    # Save to exact target filename\n",
    "    output_filename = 'tract_foot_traffic_trends_rows 1.csv'\n",
    "    final_clean_df.to_csv(output_filename, index=False)\n",
    "    print(f\"‚úÖ Saved: {output_filename}\")\n",
    "    \n",
    "    print(f\"üìä Final clean format: {final_clean_df.shape}\")\n",
    "    print(f\"üìä Columns: {len(final_clean_df.columns)} (target: 34)\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nüìã Sample of clean output:\")\n",
    "    sample_cols = ['id', 'GEOID'] + [col for col in final_clean_df.columns if '2024' in col or 'pred_2025' in col][:6]\n",
    "    print(final_clean_df[sample_cols].head())\n",
    "    \n",
    "    print(f\"\\nüéâ FIXED Pipeline Complete!\")\n",
    "    print(f\"üèÜ Best model: {best_model} (R¬≤ = {best_score:.4f})\")\n",
    "    print(f\"üìä Output ready for database!\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nüìã Sample of clean output:\")\n",
    "    sample_cols = ['id', 'GEOID'] + [col for col in final_clean_df.columns if 'pred_2025' in col or '2024' in col][:6]\n",
    "    print(final_clean_df[sample_cols].head())\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_score': best_score,\n",
    "        'predictions': final_clean_df,\n",
    "        'encoders': {'geoid': le_geoid, 'period': le_period},\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "# Run the fixed pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    results = fixed_foot_traffic_ml_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp47350py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
