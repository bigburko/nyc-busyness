{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4652ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63b45ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310 census tracts\n"
     ]
    }
   ],
   "source": [
    "# Read the GEOID\n",
    "with open(r'C:\\Users\\lukej\\OneDrive\\Desktop\\nyc-busyness\\etl\\census tract geofiles\\manhattan_census_tracts.geojson', 'r') as f:\n",
    "    geojson = json.load(f)\n",
    "    \n",
    "geoids = [feature['properties']['GEOID'] for feature in geojson['features']]\n",
    "print(f\"Found {len(geoids)} census tracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e10616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years in historical data before filtering: [2001, 2002, 2003, 2004, 2008, 2009, 2011, 2019, 2020, 2021, 2022, 2023, 2098]\n",
      "Years in historical data after filtering: [2019, 2020, 2021, 2022, 2023]\n",
      "Historical data shape: (67, 16)\n",
      "Historical columns: ['afternoon_2019', 'afternoon_2020', 'afternoon_2021', 'afternoon_2022', 'afternoon_2023']\n"
     ]
    }
   ],
   "source": [
    "# Process historical foot traffic data\n",
    "hist = pd.read_csv('foot_scores_years/all_foot_traffic_scores_with_daytime_category.csv')\n",
    "hist['year'] = pd.to_datetime(hist['trip_date']).dt.year\n",
    "\n",
    "print(\"Years in historical data before filtering:\", sorted(hist['year'].unique()))\n",
    "hist = hist[(hist['year'] >= 2019) & (hist['year'] <= 2030)]  # Keep only 2019-2023\n",
    "print(\"Years in historical data after filtering:\", sorted(hist['year'].unique()))\n",
    "\n",
    "hist['col'] = hist['daytime_category'] + '_' + hist['year'].astype(str)\n",
    "hist_wide = hist.pivot_table(index='LocationID', columns='col', values='daily_foot_traffic_score').reset_index()\n",
    "\n",
    "print(f\"Historical data shape: {hist_wide.shape}\")\n",
    "print(\"Historical columns:\", [col for col in hist_wide.columns if col != 'LocationID'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c24c7219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years in future data before filtering: [2025, 2026, 2027]\n",
      "Years in future data after filtering: [2025, 2026, 2027]\n"
     ]
    }
   ],
   "source": [
    "# Read future foot traffic data\n",
    "future = pd.read_csv('future_foot_traffic.csv')\n",
    "print(\"Years in future data before filtering:\", sorted(future['year'].unique()))\n",
    "future = future[future['year'] <= 2030]\n",
    "print(\"Years in future data after filtering:\", sorted(future['year'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dbae2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original future data: 914751 rows\n",
      "Sampled future data: 603 rows\n"
     ]
    }
   ],
   "source": [
    "# Apply random sampling approach\n",
    "# Sample one random prediction per LocationID/category/year combination\n",
    "future_sampled = future.groupby(['LocationID', 'daytime_category', 'year']).sample(n=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Original future data: {future.shape[0]} rows\")\n",
    "print(f\"Sampled future data: {future_sampled.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e0894ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future data shape: (67, 10)\n",
      "Future columns: ['afternoon_pred_2025', 'afternoon_pred_2026', 'afternoon_pred_2027', 'evening_pred_2025', 'evening_pred_2026']\n"
     ]
    }
   ],
   "source": [
    "# Create column names and pivot\n",
    "future_sampled['col'] = future_sampled['daytime_category'] + '_pred_' + future_sampled['year'].astype(str)\n",
    "future_wide = future_sampled.pivot_table(index='LocationID', columns='col', values='predicted_foot_traffic_score').reset_index()\n",
    "\n",
    "print(f\"Future data shape: {future_wide.shape}\")\n",
    "print(\"Future columns:\", [col for col in future_wide.columns if col != 'LocationID'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bebb842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions for LocationID 4.0:\n",
      "  morning_pred_2025: 1.0732\n",
      "  morning_pred_2026: 1.0308\n",
      "  morning_pred_2027: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Test if predictions vary across years\n",
    "test_cols = ['morning_pred_2025', 'morning_pred_2026', 'morning_pred_2027']\n",
    "if all(col in future_wide.columns for col in test_cols):\n",
    "    sample_location = future_wide.iloc[0]\n",
    "    print(f\"\\nSample predictions for LocationID {sample_location['LocationID']}:\")\n",
    "    for col in test_cols:\n",
    "        print(f\"  {col}: {sample_location[col]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ba0e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (67, 25)\n",
      "Unique LocationIDs: [4, 12, 13, 24, 41, 42, 43, 45, 48, 50, 68, 74, 75, 79, 87, 88, 90, 100, 103, 107, 113, 114, 116, 120, 125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246, 249, 261, 262, 263]\n",
      "All columns: ['LocationID', 'afternoon_2019', 'afternoon_2020', 'afternoon_2021', 'afternoon_2022', 'afternoon_2023', 'evening_2019', 'evening_2020', 'evening_2021', 'evening_2022', 'evening_2023', 'morning_2019', 'morning_2020', 'morning_2021', 'morning_2022', 'morning_2023', 'afternoon_pred_2025', 'afternoon_pred_2026', 'afternoon_pred_2027', 'evening_pred_2025', 'evening_pred_2026', 'evening_pred_2027', 'morning_pred_2025', 'morning_pred_2026', 'morning_pred_2027']\n"
     ]
    }
   ],
   "source": [
    "# Combine historical and future data\n",
    "foot_traffic = hist_wide.merge(future_wide, on='LocationID', how='outer').fillna(0)\n",
    "unique_locations = sorted(foot_traffic['LocationID'].unique())\n",
    "print(f\"Combined data shape: {foot_traffic.shape}\")\n",
    "print(f\"Unique LocationIDs: {unique_locations}\")\n",
    "print(\"All columns:\", foot_traffic.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73179184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping 67 LocationIDs to 310 GEOIDs\n",
      "Approximately 4 census tracts per LocationID\n",
      "Created 310 rows for GEOID mapping\n"
     ]
    }
   ],
   "source": [
    "# Map LocationIDs to GEOIDs\n",
    "result_rows = []\n",
    "locations = sorted(foot_traffic['LocationID'].unique())\n",
    "tracts_per_location = len(geoids) // len(locations)\n",
    "\n",
    "print(f\"Mapping {len(locations)} LocationIDs to {len(geoids)} GEOIDs\")\n",
    "print(f\"Approximately {tracts_per_location} census tracts per LocationID\")\n",
    "\n",
    "geoid_index = 0\n",
    "for i, location_id in enumerate(locations):\n",
    "    location_data = foot_traffic[foot_traffic['LocationID'] == location_id].iloc[0]\n",
    "    num_geoids = tracts_per_location + (1 if i < len(geoids) % len(locations) else 0)\n",
    "    \n",
    "    for j in range(num_geoids):\n",
    "        if geoid_index < len(geoids):\n",
    "            row = {'GEOID': geoids[geoid_index]}\n",
    "            for col in foot_traffic.columns:\n",
    "                if col != 'LocationID':\n",
    "                    row[col] = location_data[col]\n",
    "            result_rows.append(row)\n",
    "            geoid_index += 1\n",
    "            \n",
    "print(f\"Created {len(result_rows)} rows for GEOID mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a768c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final dataframe\n",
    "result_df = pd.DataFrame(result_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ee42ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort columns\n",
    "cols = ['GEOID'] + sorted([col for col in result_df.columns if col != 'GEOID'])\n",
    "result_df = result_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33c9c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round values and sort by GEOID\n",
    "result_df = result_df.round(2).sort_values('GEOID').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a179e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data range before normalisation:\n",
      "afternoon_2019: 1.000 - 1.000\n",
      "afternoon_2020: 1.000 - 1.620\n",
      "afternoon_2021: 1.000 - 3.820\n"
     ]
    }
   ],
   "source": [
    "# Display data before normalisation\n",
    "print(\"Current data range before normalisation:\")\n",
    "numeric_cols = [col for col in result_df.columns if col != 'GEOID']\n",
    "for col in numeric_cols[:3]:  # Sample first 3 columns\n",
    "    print(f\"{col}: {result_df[col].min():.3f} - {result_df[col].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8776c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalise to 1-10 scale\n",
    "def normalise_to_1_10(series):\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if max_val == min_val:  # Handle case where all values are the same\n",
    "        return pd.Series([5] * len(series), index=series.index)\n",
    "    \n",
    "    # Scale to 0-1, then to 1-10\n",
    "    normalised = (series - min_val) / (max_val - min_val)\n",
    "    return normalised * 9 + 1  # Scale to 1-10 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aac44605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalizing 8 morning, 8 afternoon, 8 evening columns\n"
     ]
    }
   ],
   "source": [
    "# Separate columns by time period\n",
    "morning_cols = [col for col in result_df.columns if 'morning' in col]\n",
    "afternoon_cols = [col for col in result_df.columns if 'afternoon' in col]\n",
    "evening_cols = [col for col in result_df.columns if 'evening' in col]\n",
    "\n",
    "print(f\"\\nNormalizing {len(morning_cols)} morning, {len(afternoon_cols)} afternoon, {len(evening_cols)} evening columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d654f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalised dataframe\n",
    "normalised_df = result_df[['GEOID']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "738760bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise each time period separately\n",
    "for col in morning_cols:\n",
    "    normalised_df[col] = normalise_to_1_10(result_df[col])\n",
    "    \n",
    "for col in afternoon_cols:\n",
    "    normalised_df[col] = normalise_to_1_10(result_df[col])\n",
    "\n",
    "for col in evening_cols:\n",
    "    normalised_df[col] = normalise_to_1_10(result_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "051514b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round and organize\n",
    "numeric_cols = [col for col in normalised_df.columns if col != 'GEOID']\n",
    "normalised_df[numeric_cols] = normalised_df[numeric_cols].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7207341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data ranges (1-10 scale):\n",
      "morning_2019: 5.00 - 5.00\n",
      "morning_2020: 1.00 - 10.00\n",
      "morning_2021: 1.00 - 10.00\n",
      "morning_2022: 1.00 - 10.00\n",
      "morning_2023: 5.00 - 5.00\n",
      "morning_pred_2025: 1.00 - 10.00\n"
     ]
    }
   ],
   "source": [
    "# Replace result_df with normalized version\n",
    "result_df = normalised_df\n",
    "\n",
    "print(f\"New data ranges (1-10 scale):\")\n",
    "sample_cols = [col for col in result_df.columns if col != 'GEOID'][:6]\n",
    "for col in sample_cols:\n",
    "    print(f\"{col}: {result_df[col].min():.2f} - {result_df[col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca89c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final columns: ['GEOID', 'morning_2019', 'morning_2020', 'morning_2021', 'morning_2022', 'morning_2023', 'morning_pred_2025', 'morning_pred_2026', 'morning_pred_2027', 'afternoon_2019', 'afternoon_2020', 'afternoon_2021', 'afternoon_2022', 'afternoon_2023', 'afternoon_pred_2025', 'afternoon_pred_2026', 'afternoon_pred_2027', 'evening_2019', 'evening_2020', 'evening_2021', 'evening_2022', 'evening_2023', 'evening_pred_2025', 'evening_pred_2026', 'evening_pred_2027']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFinal columns: {result_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a659c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "result_df.to_csv('brickwyze_foot_traffic-real_geoid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbf2abb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added average_2019\n",
      "Added average_2020\n",
      "Added average_2021\n",
      "Added average_2022\n",
      "Added average_2023\n",
      "Added average_pred_2025\n",
      "Added average_pred_2026\n",
      "Added average_pred_2027\n"
     ]
    }
   ],
   "source": [
    "# Fix average columns calculation\n",
    "years_to_process = ['2019', '2020', '2021', '2022', '2023', 'pred_2025', 'pred_2026', 'pred_2027']\n",
    "\n",
    "for year in years_to_process:\n",
    "    morning_col = f'morning_{year}'\n",
    "    afternoon_col = f'afternoon_{year}'\n",
    "    evening_col = f'evening_{year}'\n",
    "    avg_col = f'average_{year}'\n",
    "    \n",
    "    # Check if all three time period columns exist\n",
    "    if all(col in result_df.columns for col in [morning_col, afternoon_col, evening_col]):\n",
    "        result_df[avg_col] = (\n",
    "            result_df[morning_col] + \n",
    "            result_df[afternoon_col] + \n",
    "            result_df[evening_col]\n",
    "        ) / 3\n",
    "        print(f\"Added {avg_col}\")\n",
    "    else:\n",
    "        missing = [col for col in [morning_col, afternoon_col, evening_col] if col not in result_df.columns]\n",
    "        print(f\"Skipped {avg_col} - missing columns: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4338c23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average columns: ['average_2019', 'average_2020', 'average_2021', 'average_2022', 'average_2023', 'average_pred_2025', 'average_pred_2026', 'average_pred_2027']\n"
     ]
    }
   ],
   "source": [
    "# Round the new average columns\n",
    "avg_cols = [col for col in result_df.columns if 'average' in col]\n",
    "result_df[avg_cols] = result_df[avg_cols].round(2)\n",
    "\n",
    "print(f\"Average columns: {avg_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "243999f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated csv\n",
    "result_df.to_csv('brickwyze_foot_traffic-real_geoid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
